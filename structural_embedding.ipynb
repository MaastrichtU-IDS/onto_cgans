{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e378be34",
   "metadata": {},
   "source": [
    "## Code from OntoZSL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba349da4",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2fc82e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, triples, nentity, nrelation, negative_sample_size, mode):\n",
    "        self.len = len(triples)\n",
    "        self.triples = triples\n",
    "        self.triple_set = set(triples)\n",
    "        self.nentity = nentity\n",
    "        self.nrelation = nrelation\n",
    "        self.negative_sample_size = negative_sample_size\n",
    "        self.mode = mode\n",
    "        self.count = self.count_frequency(triples)\n",
    "        self.true_head, self.true_tail = self.get_true_head_and_tail(self.triples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        positive_sample = self.triples[idx]\n",
    "\n",
    "        head, relation, tail = positive_sample\n",
    "\n",
    "        subsampling_weight = self.count[(head, relation)] + self.count[(tail, -relation - 1)]\n",
    "        subsampling_weight = torch.sqrt(1 / torch.Tensor([subsampling_weight]))\n",
    "\n",
    "        negative_sample_list = []\n",
    "        negative_sample_size = 0\n",
    "\n",
    "        while negative_sample_size < self.negative_sample_size:\n",
    "            negative_sample = np.random.randint(self.nentity, size=self.negative_sample_size * 2)\n",
    "            if self.mode == 'head-batch':\n",
    "                mask = np.in1d(\n",
    "                    negative_sample,\n",
    "                    self.true_head[(relation, tail)],\n",
    "                    assume_unique=True,\n",
    "                    invert=True\n",
    "                )\n",
    "            elif self.mode == 'tail-batch':\n",
    "                mask = np.in1d(\n",
    "                    negative_sample,\n",
    "                    self.true_tail[(head, relation)],\n",
    "                    assume_unique=True,\n",
    "                    invert=True\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError('Training batch mode %s not supported' % self.mode)\n",
    "            negative_sample = negative_sample[mask]\n",
    "            negative_sample_list.append(negative_sample)\n",
    "            negative_sample_size += negative_sample.size\n",
    "\n",
    "        negative_sample = np.concatenate(negative_sample_list)[:self.negative_sample_size]\n",
    "\n",
    "        negative_sample = torch.from_numpy(negative_sample)\n",
    "\n",
    "        positive_sample = torch.LongTensor(positive_sample)\n",
    "\n",
    "        return positive_sample, negative_sample, subsampling_weight, self.mode\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        positive_sample = torch.stack([_[0] for _ in data], dim=0)\n",
    "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
    "        subsample_weight = torch.cat([_[2] for _ in data], dim=0)\n",
    "        mode = data[0][3]\n",
    "        return positive_sample, negative_sample, subsample_weight, mode\n",
    "\n",
    "    @staticmethod\n",
    "    def count_frequency(triples, start=4):\n",
    "        '''\n",
    "        Get frequency of a partial triple like (head, relation) or (relation, tail)\n",
    "        The frequency will be used for subsampling like word2vec\n",
    "        '''\n",
    "        count = {}\n",
    "        for head, relation, tail in triples:\n",
    "            if (head, relation) not in count:\n",
    "                count[(head, relation)] = start\n",
    "            else:\n",
    "                count[(head, relation)] += 1\n",
    "\n",
    "            if (tail, -relation - 1) not in count:\n",
    "                count[(tail, -relation - 1)] = start\n",
    "            else:\n",
    "                count[(tail, -relation - 1)] += 1\n",
    "        return count\n",
    "\n",
    "    @staticmethod\n",
    "    def get_true_head_and_tail(triples):\n",
    "        '''\n",
    "        Build a dictionary of true triples that will\n",
    "        be used to filter these true triples for negative sampling\n",
    "        '''\n",
    "\n",
    "        true_head = {}\n",
    "        true_tail = {}\n",
    "\n",
    "        for head, relation, tail in triples:\n",
    "            if (head, relation) not in true_tail:\n",
    "                true_tail[(head, relation)] = []\n",
    "            true_tail[(head, relation)].append(tail)\n",
    "            if (relation, tail) not in true_head:\n",
    "                true_head[(relation, tail)] = []\n",
    "            true_head[(relation, tail)].append(head)\n",
    "\n",
    "        for relation, tail in true_head:\n",
    "            true_head[(relation, tail)] = np.array(list(set(true_head[(relation, tail)])))\n",
    "        for head, relation in true_tail:\n",
    "            true_tail[(head, relation)] = np.array(list(set(true_tail[(head, relation)])))\n",
    "\n",
    "        return true_head, true_tail\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, triples, all_true_triples, nentity, nrelation, mode):\n",
    "        self.len = len(triples)\n",
    "        self.triple_set = set(all_true_triples)\n",
    "        self.triples = triples\n",
    "        self.nentity = nentity\n",
    "        self.nrelation = nrelation\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        head, relation, tail = self.triples[idx]\n",
    "\n",
    "        if self.mode == 'head-batch':\n",
    "            tmp = [(0, rand_head) if (rand_head, relation, tail) not in self.triple_set\n",
    "                   else (-1, head) for rand_head in range(self.nentity)]\n",
    "            tmp[head] = (0, head)\n",
    "        elif self.mode == 'tail-batch':\n",
    "            tmp = [(0, rand_tail) if (head, relation, rand_tail) not in self.triple_set\n",
    "                   else (-1, tail) for rand_tail in range(self.nentity)]\n",
    "            tmp[tail] = (0, tail)\n",
    "        else:\n",
    "            raise ValueError('negative batch mode %s not supported' % self.mode)\n",
    "\n",
    "        tmp = torch.LongTensor(tmp)\n",
    "        filter_bias = tmp[:, 0].float()\n",
    "        negative_sample = tmp[:, 1]\n",
    "\n",
    "        positive_sample = torch.LongTensor((head, relation, tail))\n",
    "\n",
    "        return positive_sample, negative_sample, filter_bias, self.mode\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        positive_sample = torch.stack([_[0] for _ in data], dim=0)\n",
    "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
    "        filter_bias = torch.stack([_[2] for _ in data], dim=0)\n",
    "        mode = data[0][3]\n",
    "        return positive_sample, negative_sample, filter_bias, mode\n",
    "\n",
    "\n",
    "class BidirectionalOneShotIterator(object):\n",
    "    def __init__(self, dataloader_head, dataloader_tail):\n",
    "        self.iterator_head = self.one_shot_iterator(dataloader_head)\n",
    "        self.iterator_tail = self.one_shot_iterator(dataloader_tail)\n",
    "        self.step = 0\n",
    "\n",
    "    def __next__(self):\n",
    "        self.step += 1\n",
    "        if self.step % 2 == 0:\n",
    "            data = next(self.iterator_head)\n",
    "        else:\n",
    "            data = next(self.iterator_tail)\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def one_shot_iterator(dataloader):\n",
    "        '''\n",
    "        Transform a PyTorch Dataloader into python iterator\n",
    "        '''\n",
    "        while True:\n",
    "            for data in dataloader:\n",
    "                yield data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a5f1e6",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0dddd25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class KGEModel(nn.Module):\n",
    "    def __init__(self, model_name, nentity, nrelation, hidden_dim, gamma,\n",
    "                 double_entity_embedding=False, double_relation_embedding=False):\n",
    "        super(KGEModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.nentity = nentity\n",
    "        self.nrelation = nrelation\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epsilon = 2.0\n",
    "\n",
    "        self.gamma = nn.Parameter(\n",
    "            torch.Tensor([gamma]),\n",
    "            requires_grad=False\n",
    "        )\n",
    "\n",
    "        self.embedding_range = nn.Parameter(\n",
    "            torch.Tensor([(self.gamma.item() + self.epsilon) / hidden_dim]),\n",
    "            requires_grad=False\n",
    "        )\n",
    "\n",
    "        self.entity_dim = hidden_dim * 2 if double_entity_embedding else hidden_dim\n",
    "        self.relation_dim = hidden_dim * 2 if double_relation_embedding else hidden_dim\n",
    "\n",
    "        self.entity_embedding = nn.Parameter(torch.zeros(nentity, self.entity_dim))\n",
    "        nn.init.uniform_(\n",
    "            tensor=self.entity_embedding,\n",
    "            a=-self.embedding_range.item(),\n",
    "            b=self.embedding_range.item()\n",
    "        )\n",
    "\n",
    "        self.relation_embedding = nn.Parameter(torch.zeros(nrelation, self.relation_dim))\n",
    "        nn.init.uniform_(\n",
    "            tensor=self.relation_embedding,\n",
    "            a=-self.embedding_range.item(),\n",
    "            b=self.embedding_range.item()\n",
    "        )\n",
    "\n",
    "        if model_name == 'pRotatE':\n",
    "            self.modulus = nn.Parameter(torch.Tensor([[0.5 * self.embedding_range.item()]]))\n",
    "\n",
    "        # Do not forget to modify this line when you add a new model in the \"forward\" function\n",
    "        if model_name not in ['TransE', 'DistMult', 'ComplEx', 'RotatE', 'pRotatE']:\n",
    "            raise ValueError('model %s not supported' % model_name)\n",
    "\n",
    "        if model_name == 'RotatE' and (not double_entity_embedding or double_relation_embedding):\n",
    "            raise ValueError('RotatE should use --double_entity_embedding')\n",
    "\n",
    "        if model_name == 'ComplEx' and (not double_entity_embedding or not double_relation_embedding):\n",
    "            raise ValueError('ComplEx should use --double_entity_embedding and --double_relation_embedding')\n",
    "\n",
    "    def forward(self, sample, mode='single'):\n",
    "        '''\n",
    "        Forward function that calculate the score of a batch of triples.\n",
    "        In the 'single' mode, sample is a batch of triple.\n",
    "        In the 'head-batch' or 'tail-batch' mode, sample consists two part.\n",
    "        The first part is usually the positive sample.\n",
    "        And the second part is the entities in the negative samples.\n",
    "        Because negative samples and positive samples usually share two elements\n",
    "        in their triple ((head, relation) or (relation, tail)).\n",
    "        '''\n",
    "        print(mode, len(sample))\n",
    "\n",
    "        if mode == 'single':\n",
    "            batch_size, negative_sample_size = sample.size(0), 1\n",
    "\n",
    "            head = torch.index_select(\n",
    "                self.entity_embedding,\n",
    "                dim=0,\n",
    "                index=sample[:, 0]\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "            relation = torch.index_select(\n",
    "                self.relation_embedding,\n",
    "                dim=0,\n",
    "                index=sample[:, 1]\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "            try:\n",
    "                tail = torch.index_select(\n",
    "                    self.entity_embedding,\n",
    "                    dim=0,\n",
    "                    index=sample[:, 2]\n",
    "                ).unsqueeze(1)\n",
    "            except IndexError:\n",
    "                print('TEST1', mode, len(sample))\n",
    "                print(sample)\n",
    "        elif mode == 'head-batch':\n",
    "            tail_part, head_part = sample\n",
    "            batch_size, negative_sample_size = head_part.size(0), head_part.size(1)\n",
    "\n",
    "            head = torch.index_select(\n",
    "                self.entity_embedding,\n",
    "                dim=0,\n",
    "                index=head_part.view(-1)\n",
    "            ).view(batch_size, negative_sample_size, -1)\n",
    "\n",
    "            relation = torch.index_select(\n",
    "                self.relation_embedding,\n",
    "                dim=0,\n",
    "                index=tail_part[:, 1]\n",
    "            ).unsqueeze(1)\n",
    "            try:\n",
    "                tail = torch.index_select(\n",
    "                    self.entity_embedding,\n",
    "                    dim=0,\n",
    "                    index=tail_part[:, 2]\n",
    "                ).unsqueeze(1)\n",
    "            except IndexError:\n",
    "                print('TEST2', mode, len(sample))\n",
    "                print(tail_part)\n",
    "\n",
    "        elif mode == 'tail-batch':\n",
    "            head_part, tail_part = sample\n",
    "            batch_size, negative_sample_size = tail_part.size(0), tail_part.size(1)\n",
    "\n",
    "            head = torch.index_select(\n",
    "                self.entity_embedding,\n",
    "                dim=0,\n",
    "                index=head_part[:, 0]\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "            relation = torch.index_select(\n",
    "                self.relation_embedding,\n",
    "                dim=0,\n",
    "                index=head_part[:, 1]\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "            tail = torch.index_select(\n",
    "                self.entity_embedding,\n",
    "                dim=0,\n",
    "                index=tail_part.view(-1)\n",
    "            ).view(batch_size, negative_sample_size, -1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('mode %s not supported' % mode)\n",
    "\n",
    "        model_func = {\n",
    "            'TransE': self.TransE,\n",
    "            'DistMult': self.DistMult,\n",
    "            'ComplEx': self.ComplEx,\n",
    "            'RotatE': self.RotatE,\n",
    "            'pRotatE': self.pRotatE\n",
    "        }\n",
    "\n",
    "        if self.model_name in model_func:\n",
    "            score = model_func[self.model_name](head, relation, tail, mode)\n",
    "        else:\n",
    "            raise ValueError('model %s not supported' % self.model_name)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def TransE(self, head, relation, tail, mode):\n",
    "\n",
    "        if mode == 'head-batch':\n",
    "            score = head + (relation - tail)\n",
    "        else:\n",
    "            score = (head + relation) - tail\n",
    "\n",
    "        score = self.gamma.item() - torch.norm(score, p=1, dim=2)\n",
    "        return score\n",
    "\n",
    "    def DistMult(self, head, relation, tail, mode):\n",
    "        if mode == 'head-batch':\n",
    "            score = head * (relation * tail)\n",
    "        else:\n",
    "            score = (head * relation) * tail\n",
    "\n",
    "        score = score.sum(dim=2)\n",
    "        return score\n",
    "\n",
    "    def ComplEx(self, head, relation, tail, mode):\n",
    "        re_head, im_head = torch.chunk(head, 2, dim=2)\n",
    "        re_relation, im_relation = torch.chunk(relation, 2, dim=2)\n",
    "        re_tail, im_tail = torch.chunk(tail, 2, dim=2)\n",
    "\n",
    "        if mode == 'head-batch':\n",
    "            re_score = re_relation * re_tail + im_relation * im_tail\n",
    "            im_score = re_relation * im_tail - im_relation * re_tail\n",
    "            score = re_head * re_score + im_head * im_score\n",
    "        else:\n",
    "            re_score = re_head * re_relation - im_head * im_relation\n",
    "            im_score = re_head * im_relation + im_head * re_relation\n",
    "            score = re_score * re_tail + im_score * im_tail\n",
    "\n",
    "        score = score.sum(dim=2)\n",
    "        return score\n",
    "\n",
    "    def RotatE(self, head, relation, tail, mode):\n",
    "        pi = 3.14159265358979323846\n",
    "\n",
    "        re_head, im_head = torch.chunk(head, 2, dim=2)\n",
    "        re_tail, im_tail = torch.chunk(tail, 2, dim=2)\n",
    "\n",
    "        # Make phases of relations uniformly distributed in [-pi, pi]\n",
    "\n",
    "        phase_relation = relation / (self.embedding_range.item() / pi)\n",
    "\n",
    "        re_relation = torch.cos(phase_relation)\n",
    "        im_relation = torch.sin(phase_relation)\n",
    "\n",
    "        if mode == 'head-batch':\n",
    "            re_score = re_relation * re_tail + im_relation * im_tail\n",
    "            im_score = re_relation * im_tail - im_relation * re_tail\n",
    "            re_score = re_score - re_head\n",
    "            im_score = im_score - im_head\n",
    "        else:\n",
    "            re_score = re_head * re_relation - im_head * im_relation\n",
    "            im_score = re_head * im_relation + im_head * re_relation\n",
    "            re_score = re_score - re_tail\n",
    "            im_score = im_score - im_tail\n",
    "\n",
    "        score = torch.stack([re_score, im_score], dim=0)\n",
    "        score = score.norm(dim=0)\n",
    "\n",
    "        score = self.gamma.item() - score.sum(dim=2)\n",
    "        return score\n",
    "\n",
    "    def pRotatE(self, head, relation, tail, mode):\n",
    "        pi = 3.14159262358979323846\n",
    "\n",
    "        # Make phases of entities and relations uniformly distributed in [-pi, pi]\n",
    "\n",
    "        phase_head = head / (self.embedding_range.item() / pi)\n",
    "        phase_relation = relation / (self.embedding_range.item() / pi)\n",
    "        phase_tail = tail / (self.embedding_range.item() / pi)\n",
    "\n",
    "        if mode == 'head-batch':\n",
    "            score = phase_head + (phase_relation - phase_tail)\n",
    "        else:\n",
    "            score = (phase_head + phase_relation) - phase_tail\n",
    "\n",
    "        score = torch.sin(score)\n",
    "        score = torch.abs(score)\n",
    "\n",
    "        score = self.gamma.item() - score.sum(dim=2) * self.modulus\n",
    "        return score\n",
    "\n",
    "    @staticmethod\n",
    "    def train_step(model, optimizer, train_iterator, args):\n",
    "        '''\n",
    "        A single train step. Apply back-propation and return the loss\n",
    "        '''\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        positive_sample, negative_sample, subsampling_weight, mode = next(train_iterator)\n",
    "\n",
    "        if args.cuda:\n",
    "            positive_sample = positive_sample.cuda()\n",
    "            negative_sample = negative_sample.cuda()\n",
    "            subsampling_weight = subsampling_weight.cuda()\n",
    "\n",
    "        negative_score = model((positive_sample, negative_sample), mode=mode)\n",
    "\n",
    "        if args.negative_adversarial_sampling:\n",
    "            # In self-adversarial sampling, we do not apply back-propagation on the sampling weight\n",
    "            negative_score = (F.softmax(negative_score * args.adversarial_temperature, dim=1).detach()\n",
    "                              * F.logsigmoid(-negative_score)).sum(dim=1)\n",
    "        else:\n",
    "            negative_score = F.logsigmoid(-negative_score).mean(dim=1)\n",
    "\n",
    "        positive_score = model(positive_sample)\n",
    "\n",
    "        positive_score = F.logsigmoid(positive_score).squeeze(dim=1)\n",
    "\n",
    "        if args.uni_weight:\n",
    "            positive_sample_loss = - positive_score.mean()\n",
    "            negative_sample_loss = - negative_score.mean()\n",
    "        else:\n",
    "            positive_sample_loss = - (subsampling_weight * positive_score).sum() / subsampling_weight.sum()\n",
    "            negative_sample_loss = - (subsampling_weight * negative_score).sum() / subsampling_weight.sum()\n",
    "\n",
    "        loss = (positive_sample_loss + negative_sample_loss) / 2\n",
    "\n",
    "        if args.regularization != 0.0:\n",
    "            # Use L3 regularization for ComplEx and DistMult\n",
    "            regularization = args.regularization * (\n",
    "                    model.entity_embedding.norm(p=3) ** 3 +\n",
    "                    model.relation_embedding.norm(p=3).norm(p=3) ** 3\n",
    "            )\n",
    "            loss = loss + regularization\n",
    "            regularization_log = {'regularization': regularization.item()}\n",
    "        else:\n",
    "            regularization_log = {}\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_values = {\n",
    "            **regularization_log,\n",
    "            'pos_sample_loss': positive_sample_loss.item(),\n",
    "            'neg_sample_loss': negative_sample_loss.item(),\n",
    "            'loss': loss.item()\n",
    "        }\n",
    "\n",
    "        return loss_values\n",
    "\n",
    "    @staticmethod\n",
    "    def test_step(model, test_triples, all_true_triples, args):\n",
    "        '''\n",
    "        Evaluate the model on test or valid datasets\n",
    "        '''\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Otherwise use standard (filtered) MRR, MR, HITS@1, HITS@3, and HITS@10 metrics\n",
    "        # Prepare dataloader for evaluation\n",
    "        test_dataloader_head = DataLoader(\n",
    "            TestDataset(\n",
    "                test_triples,\n",
    "                all_true_triples,\n",
    "                args.nentity,\n",
    "                args.nrelation,\n",
    "                'head-batch'\n",
    "            ),\n",
    "            batch_size=args.test_batch_size,\n",
    "            num_workers=max(1, args.cpu_num // 2),\n",
    "            collate_fn=TestDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        test_dataloader_tail = DataLoader(\n",
    "            TestDataset(\n",
    "                test_triples,\n",
    "                all_true_triples,\n",
    "                args.nentity,\n",
    "                args.nrelation,\n",
    "                'tail-batch'\n",
    "            ),\n",
    "            batch_size=args.test_batch_size,\n",
    "            num_workers=max(1, args.cpu_num // 2),\n",
    "            collate_fn=TestDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        test_dataset_list = [test_dataloader_head, test_dataloader_tail]\n",
    "\n",
    "        logs = []\n",
    "\n",
    "        step = 0\n",
    "        total_steps = sum([len(dataset) for dataset in test_dataset_list])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for test_dataset in test_dataset_list:\n",
    "                for positive_sample, negative_sample, filter_bias, mode in test_dataset:\n",
    "                    if args.cuda:\n",
    "                        positive_sample = positive_sample.cuda()\n",
    "                        negative_sample = negative_sample.cuda()\n",
    "                        filter_bias = filter_bias.cuda()\n",
    "\n",
    "                    batch_size = positive_sample.size(0)\n",
    "\n",
    "                    score = model((positive_sample, negative_sample), mode)\n",
    "                    score += filter_bias\n",
    "\n",
    "                    # Explicitly sort all the entities to ensure that there is no test exposure bias\n",
    "                    argsort = torch.argsort(score, dim=1, descending=True)\n",
    "\n",
    "                    if mode == 'head-batch':\n",
    "                        positive_arg = positive_sample[:, 0]\n",
    "                    elif mode == 'tail-batch':\n",
    "                        positive_arg = positive_sample[:, 2]\n",
    "                    else:\n",
    "                        raise ValueError('mode %s not supported' % mode)\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        # Notice that argsort is not ranking\n",
    "                        ranking = (argsort[i, :] == positive_arg[i]).nonzero()\n",
    "                        assert ranking.size(0) == 1\n",
    "\n",
    "                        # ranking + 1 is the true ranking used in evaluation metrics\n",
    "                        ranking = 1 + ranking.item()\n",
    "                        logs.append({\n",
    "                            'MRR': 1.0 / ranking,\n",
    "                            'MR': float(ranking),\n",
    "                            'HITS@1': 1.0 if ranking <= 1 else 0.0,\n",
    "                            'HITS@3': 1.0 if ranking <= 3 else 0.0,\n",
    "                            'HITS@10': 1.0 if ranking <= 10 else 0.0,\n",
    "                        })\n",
    "\n",
    "                    if step % args.test_log_steps == 0:\n",
    "                        logging.info('Evaluating the model... (%d/%d)' % (step, total_steps))\n",
    "\n",
    "                    step += 1\n",
    "\n",
    "        metrics = {}\n",
    "        for metric in logs[0].keys():\n",
    "            metrics[metric] = sum([log[metric] for log in logs]) / len(logs)\n",
    "\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b65fe0f",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e8efe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if __name__ == '__main__':\\n\\n    datadir = '../../data'\\n\\n    # dataset = 'AwA'\\n    dataset = 'ImageNet/ImNet_A'\\n\\n\\n\\n    DATASET_DIR = os.path.join(datadir, dataset)\\n    DATA_DIR = os.path.join(datadir, dataset, 'onto_file')\\n\\n\\n    # load entity dict\\n    entity_file = os.path.join(DATA_DIR, 'entities.dict')\\n    entities = loadDict(entity_file)\\n\\n    embed_dir = os.path.join(DATA_DIR, 'save_onto_embeds')\\n\\n    embed_file = embed_dir + '/entity_55000.npy'\\n\\n\\n    save_file = 'o2v-55000.mat'\\n\\n    if dataset == 'AwA':\\n        wnids = train_wnid + test_wnid\\n        names = train_name + test_name\\n        save_embed_awa(embed_file, wnids, names)\\n\\n    else:\\n        seen_file = os.path.join(DATASET_DIR, 'seen.txt')\\n        unseen_file = os.path.join(DATASET_DIR, 'unseen.txt')\\n        seen, unseen = load_class()\\n        classes = seen + unseen\\n\\n        save_embed(embed_file, classes)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "\n",
    "\n",
    "\n",
    "def readTxt(file_name):\n",
    "    class_list = list()\n",
    "    wnids = open(file_name, 'rU')\n",
    "    try:\n",
    "        for line in wnids:\n",
    "            line = line[:-1]\n",
    "            class_list.append(line)\n",
    "    finally:\n",
    "        wnids.close()\n",
    "    print(len(class_list))\n",
    "    return class_list\n",
    "\n",
    "def load_class():\n",
    "    seen = readTxt(seen_file)\n",
    "    unseen = readTxt(unseen_file)\n",
    "    return seen, unseen\n",
    "\n",
    "###########################\n",
    "\n",
    "def loadDict(file_name):\n",
    "    entities = list()\n",
    "    wnids = open(file_name, 'rU')\n",
    "    try:\n",
    "        for line in wnids:\n",
    "            line = line[:-1]\n",
    "            index, cls = line.split('\\t')\n",
    "            entities.append(cls)\n",
    "    finally:\n",
    "        wnids.close()\n",
    "    print(len(entities))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def save_embed_awa(filename, wnids, names):\n",
    "\n",
    "    # load embeddings\n",
    "    embeds = np.load(filename)\n",
    "    # save to .mat file\n",
    "    matcontent = scio.loadmat(os.path.join(DATASET_DIR, 'att_splits.mat'))\n",
    "    all_names = matcontent['allclasses_names'].squeeze().tolist()\n",
    "\n",
    "    embed_size = embeds.shape[1]\n",
    "    o2v = np.zeros((len(all_names), embed_size), dtype=np.float)\n",
    "    for i in range(len(all_names)):\n",
    "        name = all_names[i][0]\n",
    "        wnid = wnids[names.index(name)]\n",
    "        o2v[i] = embeds[entities.index(wnid)]\n",
    "\n",
    "    print(o2v.shape)\n",
    "\n",
    "    o2v_file = os.path.join(DATA_DIR, save_file)\n",
    "    scio.savemat(o2v_file, {'o2v': o2v})\n",
    "\n",
    "def save_embed(filename, classes):\n",
    "\n",
    "    # load embeddings\n",
    "    embeds = np.load(filename)\n",
    "    # save to .mat file\n",
    "    matcontent = scio.loadmat(os.path.join(datadir, 'ImageNet', 'w2v.mat'))\n",
    "    wnids = matcontent['wnids'].squeeze().tolist()\n",
    "    wnids = wnids[:2549]\n",
    "    embed_size = embeds.shape[1]\n",
    "    o2v = np.zeros((len(wnids), embed_size), dtype=np.float)\n",
    "\n",
    "    print(o2v.shape)\n",
    "    for i, wnid in enumerate(wnids):\n",
    "        wnid = wnid[0]\n",
    "        if wnid in classes:\n",
    "            o2v[i] = embeds[entities.index(wnid)]\n",
    "        else:\n",
    "            continue\n",
    "    # save wnids together\n",
    "    wnids_cell = np.empty((len(wnids), 1), dtype=np.object)\n",
    "    for i in range(len(wnids)):\n",
    "        wnids_cell[i][0] = np.array(wnids[i])\n",
    "\n",
    "    o2v_file = os.path.join(DATA_DIR, save_file)\n",
    "    scio.savemat(o2v_file, {'o2v': o2v, 'wnids': wnids_cell})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8042697",
   "metadata": {},
   "source": [
    "### Training the model for the structural embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "741a965c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#total triples num: 573708\n",
      "Ramdomly Initializing TransE Model...\n",
      "------ Start Training...\n",
      "batch_size = 256\n",
      "negative sample size = 1024\n",
      "hidden_dim = 100\n",
      "gamma = 12.000000\n",
      "negative_adversarial_sampling = True\n",
      "adversarial_temperature = 1.000000\n",
      "learning rate = 0.000050\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "tail-batch 2\n",
      "single 256\n",
      "head-batch 2\n",
      "single 256\n",
      "Training Step: 100; average -> pos_sample_loss: 0.560977; neg_sample_loss: 1.325958; loss: 0.943468\n",
      "------ Evaluating on Training Dataset...\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n",
      "head-batch 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 271>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    268\u001b[0m                 log_metrics(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m, step, metrics)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# random_seed = random.randint(1, 10000)\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# random_seed = 5487\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# torch.manual_seed(random_seed)\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# torch.cuda.manual_seed_all(random_seed)\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--max_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m500\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--valid_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m100\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mevaluate_train \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mvalid_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m------ Evaluating on Training Dataset...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 267\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mkge_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkge_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_triples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_true_triples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     log_metrics(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m, step, metrics)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mKGEModel.test_step\u001b[0;34m(model, test_triples, all_true_triples, args)\u001b[0m\n\u001b[1;32m    367\u001b[0m     filter_bias \u001b[38;5;241m=\u001b[39m filter_bias\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    369\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m positive_sample\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 371\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_sample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m filter_bias\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# Explicitly sort all the entities to ensure that there is no test exposure bias\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/4A/stage/onto_cgans/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mKGEModel.forward\u001b[0;34m(self, sample, mode)\u001b[0m\n\u001b[1;32m    154\u001b[0m model_func \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTransE,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistMult\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDistMult,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpRotatE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpRotatE\n\u001b[1;32m    160\u001b[0m }\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;129;01min\u001b[39;00m model_func:\n\u001b[0;32m--> 163\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_func\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mKGEModel.TransE\u001b[0;34m(self, head, relation, tail, mode)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     score \u001b[38;5;241m=\u001b[39m (head \u001b[38;5;241m+\u001b[39m relation) \u001b[38;5;241m-\u001b[39m tail\n\u001b[0;32m--> 176\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "File \u001b[0;32m~/Documents/4A/stage/onto_cgans/venv/lib/python3.9/site-packages/torch/functional.py:1590\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1590\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdim\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1592\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28minput\u001b[39m, p, _dim, keepdim\u001b[38;5;241m=\u001b[39mkeepdim, dtype\u001b[38;5;241m=\u001b[39mdtype)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "# import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Training and Testing Knowledge Graph Embedding Models',\n",
    "        usage='train.py [<args>] [-h | --help]'\n",
    "    )\n",
    "\n",
    "    parser.add_argument('--cuda', action='store_true', help='use GPU', default=False)\n",
    "    parser.add_argument('--CUDA_DEVISE', default='1', help='')\n",
    "\n",
    "    parser.add_argument('--do_train', action='store_true', default=True)\n",
    "    parser.add_argument('--do_valid', action='store_true')\n",
    "    parser.add_argument('--do_test', action='store_true')\n",
    "    parser.add_argument('--evaluate_train', action='store_true', help='Evaluate on training data', default=True)\n",
    "\n",
    "    parser.add_argument('--datadir', type=str, default='data')\n",
    "    parser.add_argument('--dataset', type=str, default='ontology')\n",
    "\n",
    "    parser.add_argument('-save', '--save_path', type=str)\n",
    "\n",
    "    parser.add_argument('--model', default='TransE', type=str)\n",
    "    parser.add_argument('-de', '--double_entity_embedding', action='store_true')\n",
    "    parser.add_argument('-dr', '--double_relation_embedding', action='store_true')\n",
    "\n",
    "    parser.add_argument('-n', '--negative_sample_size', default=1024, type=int)\n",
    "    parser.add_argument('-d', '--hidden_dim', default=100, type=int)\n",
    "    parser.add_argument('-g', '--gamma', default=12, type=float)\n",
    "    parser.add_argument('-adv', '--negative_adversarial_sampling', action='store_true', default=True)\n",
    "    parser.add_argument('-a', '--adversarial_temperature', default=1, type=float)\n",
    "    parser.add_argument('-b', '--batch_size', default=256, type=int)\n",
    "    parser.add_argument('-r', '--regularization', default=0.0, type=float)\n",
    "    parser.add_argument('--test_batch_size', default=8, type=int, help='valid/test batch size')\n",
    "    parser.add_argument('--uni_weight', action='store_true',\n",
    "                        help='Otherwise use subsampling weighting like in word2vec')\n",
    "\n",
    "    parser.add_argument('-lr', '--learning_rate', default=0.00005, type=float)\n",
    "    parser.add_argument('-cpu', '--cpu_num', default=10, type=int)\n",
    "\n",
    "\n",
    "    parser.add_argument('--max_steps', default=80000, type=int)\n",
    "    parser.add_argument('--warm_up_steps', default=None, type=int)\n",
    "\n",
    "    parser.add_argument('--save_steps', default=1000, type=int)\n",
    "    parser.add_argument('--valid_steps', default=1000, type=int)\n",
    "    parser.add_argument('--print_steps', default=100, type=int, help='train log every xx steps')\n",
    "    parser.add_argument('--test_log_steps', default=1000, type=int, help='valid/test log every xx steps')\n",
    "\n",
    "    parser.add_argument('--nentity', type=int, default=0, help='DO NOT MANUALLY SET')\n",
    "    parser.add_argument('--nrelation', type=int, default=0, help='DO NOT MANUALLY SET')\n",
    "\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "def save_embeddings(model, step, args):\n",
    "    '''\n",
    "    Save the parameters of the model and the optimizer,\n",
    "    as well as some other variables such as step and learning_rate\n",
    "    '''\n",
    "    file_name = 'entity_' + str(step)\n",
    "    entity_embedding = model.entity_embedding.detach().cpu().numpy()\n",
    "\n",
    "    np.save(\n",
    "        os.path.join(args.save_path, file_name),\n",
    "        entity_embedding\n",
    "    )\n",
    "\n",
    "    rel_file_name = 'relation_' + str(step)\n",
    "    relation_embedding = model.relation_embedding.detach().cpu().numpy()\n",
    "    np.save(\n",
    "        os.path.join(args.save_path, rel_file_name),\n",
    "        relation_embedding\n",
    "    )\n",
    "\n",
    "\n",
    "def read_triple(file_path, entity2id, relation2id):\n",
    "    '''\n",
    "    Read triples and map them into ids.\n",
    "    '''\n",
    "    triples = []\n",
    "    with open(file_path) as fin:\n",
    "        for line in fin:\n",
    "            h, r, t = line.strip().split('\\t')\n",
    "            triples.append((entity2id[h], relation2id[r], entity2id[t]))\n",
    "    return triples\n",
    "\n",
    "\n",
    "def log_metrics(mode, step, metrics):\n",
    "    '''\n",
    "    Print the evaluation logs\n",
    "    '''\n",
    "    for metric in metrics:\n",
    "        print('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.CUDA_DEVISE\n",
    "\n",
    "    args.data_path = args.datadir\n",
    "\n",
    "\n",
    "\n",
    "    # if args.init_checkpoint:\n",
    "    #     override_config(args)\n",
    "    if args.data_path is None:\n",
    "        raise ValueError('data_path and dataset must be choosed.')\n",
    "\n",
    "    args.save_path = os.path.join(args.data_path, 'save_onto_embeds')\n",
    "\n",
    "    # if args.do_train and args.save_path is None:\n",
    "    #     raise ValueError('Where do you want to save your trained model?')\n",
    "\n",
    "    if args.save_path and not os.path.exists(args.save_path):\n",
    "        os.makedirs(args.save_path)\n",
    "\n",
    "    with open(os.path.join(args.data_path, 'entities.dict')) as fin:\n",
    "        entity2id = dict()\n",
    "        for line in fin:\n",
    "            eid, entity = line.strip().split('\\t')\n",
    "            entity2id[entity] = int(eid)\n",
    "\n",
    "    with open(os.path.join(args.data_path, 'relations.dict')) as fin:\n",
    "        relation2id = dict()\n",
    "        for line in fin:\n",
    "            rid, relation = line.strip().split('\\t')\n",
    "            relation2id[relation] = int(rid)\n",
    "\n",
    "    nentity = len(entity2id)\n",
    "    nrelation = len(relation2id)\n",
    "\n",
    "    args.nentity = nentity\n",
    "    args.nrelation = nrelation\n",
    "\n",
    "    print('Model: %s' % args.model)\n",
    "    # print('Data Path: %s' % args.data_path + \"/\" + args.dataset)\n",
    "    print('#entity num: %d' % nentity)\n",
    "    print('#relation num: %d' % nrelation)\n",
    "\n",
    "    print(entity2id)\n",
    "    all_triples = read_triple(os.path.join(args.data_path, 'triples.txt'), entity2id,\n",
    "                              relation2id)\n",
    "    print('#total triples num: %d' % len(all_triples))\n",
    "\n",
    "\n",
    "    # All true triples\n",
    "    all_true_triples = all_triples\n",
    "\n",
    "    kge_model = KGEModel(\n",
    "        model_name=args.model,\n",
    "        nentity=nentity,\n",
    "        nrelation=nrelation,\n",
    "        hidden_dim=args.hidden_dim,\n",
    "        gamma=args.gamma,\n",
    "        double_entity_embedding=args.double_entity_embedding,\n",
    "        double_relation_embedding=args.double_relation_embedding\n",
    "    )\n",
    "\n",
    "    # logging.info('Model Parameter Configuration:')\n",
    "    # for name, param in kge_model.named_parameters():\n",
    "    #     logging.info('Parameter %s: %s, require_grad = %s' % (name, str(param.size()), str(param.requires_grad)))\n",
    "\n",
    "    #if args.cuda:\n",
    "    #    kge_model = kge_model.cuda()\n",
    "\n",
    "    if args.do_train:\n",
    "        # Set training dataloader iterator\n",
    "        train_dataloader_head = DataLoader(\n",
    "            TrainDataset(all_triples, nentity, nrelation, args.negative_sample_size, 'head-batch'),\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=max(1, args.cpu_num // 2),\n",
    "            collate_fn=TrainDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        train_dataloader_tail = DataLoader(\n",
    "            TrainDataset(all_triples, nentity, nrelation, args.negative_sample_size, 'tail-batch'),\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=max(1, args.cpu_num // 2),\n",
    "            collate_fn=TrainDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        train_iterator = BidirectionalOneShotIterator(train_dataloader_head, train_dataloader_tail)\n",
    "\n",
    "        # Set training configuration\n",
    "        current_learning_rate = args.learning_rate\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, kge_model.parameters()),\n",
    "            lr=current_learning_rate\n",
    "        )\n",
    "        if args.warm_up_steps:\n",
    "            warm_up_steps = args.warm_up_steps\n",
    "        else:\n",
    "            warm_up_steps = args.max_steps // 2\n",
    "\n",
    "    print('Ramdomly Initializing %s Model...' % args.model)\n",
    "\n",
    "    # step = init_step\n",
    "\n",
    "    print('------ Start Training...')\n",
    "    print('batch_size = %d' % args.batch_size)\n",
    "    print('negative sample size = %d' % args.negative_sample_size)\n",
    "    print('hidden_dim = %d' % args.hidden_dim)\n",
    "    print('gamma = %f' % args.gamma)\n",
    "    print('negative_adversarial_sampling = %s' % str(args.negative_adversarial_sampling))\n",
    "\n",
    "    if args.negative_adversarial_sampling:\n",
    "        print('adversarial_temperature = %f' % args.adversarial_temperature)\n",
    "\n",
    "    print(\"learning rate = %f\" % current_learning_rate)\n",
    "\n",
    "    # Set valid dataloader as it would be evaluated during training\n",
    "\n",
    "    if args.do_train:\n",
    "\n",
    "        train_losses = []\n",
    "\n",
    "        # Training Loop\n",
    "        for step in range(1, args.max_steps + 1):\n",
    "\n",
    "            loss_values = kge_model.train_step(kge_model, optimizer, train_iterator, args)\n",
    "\n",
    "            train_losses.append(loss_values)\n",
    "\n",
    "            if step >= warm_up_steps:\n",
    "                current_learning_rate = current_learning_rate / 10\n",
    "                print('Change learning_rate to %f at step %d' % (current_learning_rate, step))\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    filter(lambda p: p.requires_grad, kge_model.parameters()),\n",
    "                    lr=current_learning_rate\n",
    "                )\n",
    "                warm_up_steps = warm_up_steps * 3\n",
    "\n",
    "            if step % args.print_steps == 0:\n",
    "                pos_sample_loss = sum([losses['pos_sample_loss'] for losses in train_losses]) / len(train_losses)\n",
    "                neg_sample_loss = sum([losses['neg_sample_loss'] for losses in train_losses]) / len(train_losses)\n",
    "                loss1 = sum([losses['loss'] for losses in train_losses]) / len(train_losses)\n",
    "\n",
    "                # log_metrics('Training average', step, metrics)\n",
    "                print('Training Step: %d; average -> pos_sample_loss: %f; neg_sample_loss: %f; loss: %f' %\n",
    "                      (step, pos_sample_loss, neg_sample_loss, loss1))\n",
    "                train_losses = []\n",
    "\n",
    "            if step % args.save_steps == 0:\n",
    "                save_embeddings(kge_model, step, args)\n",
    "\n",
    "            if args.evaluate_train and step % args.valid_steps == 0:\n",
    "                print('------ Evaluating on Training Dataset...')\n",
    "                metrics = kge_model.test_step(kge_model, all_triples, all_true_triples, args)\n",
    "                log_metrics('Test', step, metrics)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # random_seed = random.randint(1, 10000)\n",
    "    # random_seed = 5487\n",
    "    #\n",
    "    # print(\"random seed:\", random_seed)\n",
    "    # random.seed(random_seed)\n",
    "    # torch.manual_seed(random_seed)\n",
    "    # torch.cuda.manual_seed_all(random_seed)\n",
    "    main(parse_args(args=['--max_steps', '500', '--save-steps', '500', '--valid_steps', '100', '--evaluate_train', 'False']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onto_cgans",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
