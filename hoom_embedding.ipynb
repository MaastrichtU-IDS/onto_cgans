{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook from machine-learning-with-ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 13:01:08.228438: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-10 13:01:08.228454: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import click as ck\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import function\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.stats import rankdata\n",
    "import os\n",
    "\n",
    "from elembeddings.elembedding import (\n",
    "    ELModel, load_data, load_valid_data, Generator, MyModelCheckpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 256\n",
    "embedding_size = 50\n",
    "margin = -0.1\n",
    "reg_norm = 1\n",
    "learning_rate = 1e-3\n",
    "epochs = 20\n",
    "org_id = '9606'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for key, value in train_data.items():\\n    print(f'{key} {len(value)}')\\nfor key, value in classes.items():\\n    print(f'{key}: {value}')\\nfor key, value in relations.items():\\n    print(f'{key}: {value}')\\nfor h, l, t in valid_data:\\n    print(f'{h}: {l}: {t}')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load training data in (h, l, t) triples\n",
    "# classes and relations are entity to id mappings\n",
    "train_data, classes, relations = load_data(f'data/train/{org_id}.classes-normalized.owl')\n",
    "valid_data = load_valid_data(f'data/valid/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "\"\"\"for key, value in train_data.items():\n",
    "    print(f'{key} {len(value)}')\n",
    "for key, value in classes.items():\n",
    "    print(f'{key}: {value}')\n",
    "for key, value in relations.items():\n",
    "    print(f'{key}: {value}')\n",
    "for h, l, t in valid_data:\n",
    "    print(f'{h}: {l}: {t}')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes 61810\n",
      "Total number of relations 10\n"
     ]
    }
   ],
   "source": [
    "# Filter out protein classes\n",
    "proteins = {}\n",
    "for k, v in classes.items():\n",
    "    if not k.startswith('<http://purl.obolibrary.org/obo/GO_'):\n",
    "        proteins[k] = v\n",
    "\n",
    "# Prepare data for training the model\n",
    "nb_classes = len(classes)\n",
    "nb_relations = len(relations)\n",
    "nb_train_data = 0\n",
    "for key, val in train_data.items():\n",
    "    nb_train_data = max(len(val), nb_train_data)\n",
    "train_steps = int(math.ceil(nb_train_data / (1.0 * batch_size)))\n",
    "train_generator = Generator(train_data, batch_size, steps=train_steps)\n",
    "\n",
    "# id to entity maps\n",
    "cls_dict = {v: k for k, v in classes.items()}\n",
    "rel_dict = {v: k for k, v in relations.items()}\n",
    "\n",
    "cls_list = []\n",
    "rel_list = []\n",
    "for i in range(nb_classes):\n",
    "    cls_list.append(cls_dict[i])\n",
    "for i in range(nb_relations):\n",
    "    rel_list.append(rel_dict[i])\n",
    "\n",
    "        \n",
    "print('Total number of classes', nb_classes)\n",
    "print('Total number of relations', nb_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ELEmbeddings Model and Train\n",
    "\n",
    "Embeddings are saved depending on mean rank evaluation on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 13:01:25.243367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-10 13:01:25.243839: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-10 13:01:25.243951: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-10 13:01:25.244043: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-10 13:01:25.244136: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-10 13:01:25.244225: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-10 13:01:25.244316: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-10 13:01:25.244406: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-10 13:01:25.244494: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-10 13:01:25.244509: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-10 13:01:25.244884: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello2\n",
      "Epoch 1/20\n",
      "4206/4206 [==============================] - ETA: 0s - loss: 14.2395\n",
      " Validation 1 4318.369714409735\n",
      "\n",
      "\n",
      " Saving embeddings 1 4318.369714409735\n",
      "\n",
      "4206/4206 [==============================] - 639s 151ms/step - loss: 14.2395\n",
      "Epoch 2/20\n",
      "4205/4206 [============================>.] - ETA: 0s - loss: 1.3521\n",
      " Validation 2 1866.987419359667\n",
      "\n",
      "\n",
      " Saving embeddings 2 1866.987419359667\n",
      "\n",
      "4206/4206 [==============================] - 624s 148ms/step - loss: 1.3519\n",
      "Epoch 3/20\n",
      "4206/4206 [==============================] - ETA: 0s - loss: 0.2944\n",
      " Validation 3 2674.5984710143844\n",
      "\n",
      "4206/4206 [==============================] - 617s 147ms/step - loss: 0.2944\n",
      "Epoch 4/20\n",
      "4206/4206 [==============================] - ETA: 0s - loss: 0.1200\n",
      " Validation 4 4251.966587585505\n",
      "\n",
      "4206/4206 [==============================] - 612s 146ms/step - loss: 0.1200\n",
      "Epoch 5/20\n",
      "4206/4206 [==============================] - ETA: 0s - loss: 0.0815\n",
      " Validation 5 5054.082646050681\n",
      "\n",
      "4206/4206 [==============================] - 623s 148ms/step - loss: 0.0815\n",
      "Epoch 6/20\n",
      "4205/4206 [============================>.] - ETA: 0s - loss: 0.0681\n",
      " Validation 6 5561.519645556737\n",
      "\n",
      "4206/4206 [==============================] - 633s 150ms/step - loss: 0.0681\n",
      "Epoch 7/20\n",
      "4205/4206 [============================>.] - ETA: 0s - loss: 0.0625\n",
      " Validation 7 5816.805647442709\n",
      "\n",
      "4206/4206 [==============================] - 667s 159ms/step - loss: 0.0625\n",
      "Epoch 8/20\n",
      "4206/4206 [==============================] - ETA: 0s - loss: 0.0591\n",
      " Validation 8 5983.575083446841\n",
      "\n",
      "4206/4206 [==============================] - 654s 156ms/step - loss: 0.0591\n",
      "Epoch 9/20\n",
      "4205/4206 [============================>.] - ETA: 0s - loss: 0.0572\n",
      " Validation 9 6016.2081081890165\n",
      "\n",
      "4206/4206 [==============================] - 662s 154ms/step - loss: 0.0572\n",
      "Epoch 10/20\n",
      "4205/4206 [============================>.] - ETA: 0s - loss: 0.0560\n",
      " Validation 10 6118.445684713138\n",
      "\n",
      "4206/4206 [==============================] - 668s 159ms/step - loss: 0.0560\n",
      "Epoch 11/20\n",
      "4206/4206 [==============================] - ETA: 0s - loss: 0.0549\n",
      " Validation 11 6073.792146267719\n",
      "\n",
      "4206/4206 [==============================] - 684s 163ms/step - loss: 0.0549\n",
      "Epoch 12/20\n",
      "4205/4206 [============================>.] - ETA: 0s - loss: 0.0542\n",
      " Validation 12 6112.11101797662\n",
      "\n",
      "4206/4206 [==============================] - 738s 175ms/step - loss: 0.0542\n",
      "Epoch 13/20\n",
      "4206/4206 [==============================] - ETA: 0s - loss: 0.0535\n",
      " Validation 13 6159.192953045248\n",
      "\n",
      "4206/4206 [==============================] - 835s 199ms/step - loss: 0.0535\n",
      "Epoch 14/20\n",
      "4206/4206 [==============================] - ETA: 0s - loss: 0.0530\n",
      " Validation 14 6259.334513314075\n",
      "\n",
      "4206/4206 [==============================] - 670s 159ms/step - loss: 0.0530\n",
      "Epoch 15/20\n",
      "4206/4206 [==============================] - ETA: 0s - loss: 0.0525\n",
      " Validation 15 6170.815967908516\n",
      "\n",
      "4206/4206 [==============================] - 655s 156ms/step - loss: 0.0525\n",
      "Epoch 16/20\n",
      "4205/4206 [============================>.] - ETA: 0s - loss: 0.0519\n",
      " Validation 16 6181.351861276175\n",
      "\n",
      "4206/4206 [==============================] - 658s 156ms/step - loss: 0.0519\n",
      "Epoch 17/20\n",
      "4206/4206 [==============================] - ETA: 0s - loss: 0.0515\n",
      " Validation 17 6113.945868071667\n",
      "\n",
      "4206/4206 [==============================] - 628s 149ms/step - loss: 0.0515\n",
      "Epoch 18/20\n",
      "4205/4206 [============================>.] - ETA: 0s - loss: 0.0511\n",
      " Validation 18 6138.80838285261\n",
      "\n",
      "4206/4206 [==============================] - 629s 149ms/step - loss: 0.0511\n",
      "Epoch 19/20\n",
      "4205/4206 [============================>.] - ETA: 0s - loss: 0.0507\n",
      " Validation 19 6214.847977068957\n",
      "\n",
      "4206/4206 [==============================] - 668s 159ms/step - loss: 0.0507\n",
      "Epoch 20/20\n",
      "4205/4206 [============================>.] - ETA: 0s - loss: 0.0502\n",
      " Validation 20 6165.627209657381\n",
      "\n",
      "4206/4206 [==============================] - 670s 159ms/step - loss: 0.0502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f63a36b8220>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input layers for each loss type\n",
    "nf1 = Input(shape=(2,), dtype=np.int32)\n",
    "nf2 = Input(shape=(3,), dtype=np.int32)\n",
    "nf3 = Input(shape=(3,), dtype=np.int32)\n",
    "nf4 = Input(shape=(3,), dtype=np.int32)\n",
    "dis = Input(shape=(3,), dtype=np.int32)\n",
    "top = Input(shape=(1,), dtype=np.int32)\n",
    "nf3_neg = Input(shape=(3,), dtype=np.int32)\n",
    "\n",
    "# Build model\n",
    "el_model = ELModel(nb_classes, nb_relations, embedding_size, batch_size, margin, reg_norm)\n",
    "out = el_model([nf1, nf2, nf3, nf4, dis, top, nf3_neg])\n",
    "model = tf.keras.Model(inputs=[nf1, nf2, nf3, nf4, dis, top, nf3_neg], outputs=out)\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Pandas files to store embeddings\n",
    "if not os.path.exists('data/elembeddings'):\n",
    "    os.makedirs('data/elembeddings')\n",
    "out_classes_file = f'data/elembeddings/{org_id}_cls_embeddings.pkl'\n",
    "out_relations_file = f'data/elembeddings/{org_id}_rel_embeddings.pkl'\n",
    "\n",
    "# ModelCheckpoint which runs at the end of each epoch\n",
    "checkpointer = MyModelCheckpoint(\n",
    "    save_freq='epoch',\n",
    "    out_classes_file=out_classes_file,\n",
    "    out_relations_file=out_relations_file,\n",
    "    cls_list=cls_list,\n",
    "    rel_list=rel_list,\n",
    "    valid_data=valid_data,\n",
    "    proteins=proteins,\n",
    "    monitor='loss')\n",
    "\n",
    "# Start training\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    workers=12,\n",
    "    callbacks=[checkpointer,])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of embeddings on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 9606\n",
      "0.01 0.14 1800.38 0.89\n",
      "0.01 0.15 1766.14 0.89\n"
     ]
    }
   ],
   "source": [
    "def load_test_data(data_file, classes, relations):\n",
    "    data = []\n",
    "    rel = f'<http://interacts>'\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = f'<http://{it[0]}>'\n",
    "            id2 = f'<http://{it[1]}>'\n",
    "            if id1 not in classes or id2 not in classes or rel not in relations:\n",
    "                continue\n",
    "            data.append((id1, rel, id2))\n",
    "    return data\n",
    "\n",
    "def compute_rank_roc(ranks, n_prots):\n",
    "    auc_x = list(ranks.keys())\n",
    "    auc_x.sort()\n",
    "    auc_y = []\n",
    "    tpr = 0\n",
    "    sum_rank = sum(ranks.values())\n",
    "    for x in auc_x:\n",
    "        tpr += ranks[x]\n",
    "        auc_y.append(tpr / sum_rank)\n",
    "    auc_x.append(n_prots)\n",
    "    auc_y.append(1)\n",
    "    auc = np.trapz(auc_y, auc_x) / n_prots\n",
    "    return auc\n",
    "\n",
    "\n",
    "# Pandas files to store embeddings\n",
    "out_classes_file = f'data/elembeddings/{org_id}_cls_embeddings.pkl'\n",
    "out_relations_file = f'data/elembeddings/{org_id}_rel_embeddings.pkl'\n",
    "\n",
    "cls_df = pd.read_pickle(out_classes_file)\n",
    "rel_df = pd.read_pickle(out_relations_file)\n",
    "nb_classes = len(cls_df)\n",
    "nb_relations = len(rel_df)\n",
    "embeds_list = cls_df['embeddings'].values\n",
    "rembeds_list = rel_df['embeddings'].values\n",
    "size = len(embeds_list[0])\n",
    "embeds = np.zeros((nb_classes, size), dtype=np.float32)\n",
    "for i, emb in enumerate(embeds_list):\n",
    "    embeds[i, :] = emb\n",
    "\n",
    "rs = np.abs(embeds[:, -1]).reshape(-1, 1)\n",
    "embeds = embeds[:, :-1]\n",
    "prot_index = list(proteins.values())\n",
    "prot_rs = rs[prot_index, :]\n",
    "prot_embeds = embeds[prot_index, :]\n",
    "prot_dict = {v: k for k, v in enumerate(prot_index)}\n",
    "    \n",
    "rsize = len(rembeds_list[0])\n",
    "rembeds = np.zeros((nb_relations, rsize), dtype=np.float32)\n",
    "for i, emb in enumerate(rembeds_list):\n",
    "    rembeds[i, :] = emb\n",
    "\n",
    "train_data = load_test_data(f'data/train/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "valid_data = load_test_data(f'data/valid/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "trlabels = {}\n",
    "for c, r, d in train_data:\n",
    "    c, r, d = prot_dict[classes[c]], relations[r], prot_dict[classes[d]]\n",
    "    if r not in trlabels:\n",
    "        trlabels[r] = np.ones((len(prot_embeds), len(prot_embeds)), dtype=np.int32)\n",
    "    trlabels[r][c, d] = 1000\n",
    "for c, r, d in valid_data:\n",
    "    c, r, d = prot_dict[classes[c]], relations[r], prot_dict[classes[d]]\n",
    "    if r not in trlabels:\n",
    "        trlabels[r] = np.ones((len(prot_embeds), len(prot_embeds)), dtype=np.int32)\n",
    "    trlabels[r][c, d] = 1000\n",
    "\n",
    "test_data = load_test_data(f'data/test/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "top1 = 0\n",
    "top10 = 0\n",
    "top100 = 0\n",
    "mean_rank = 0\n",
    "ftop1 = 0\n",
    "ftop10 = 0\n",
    "ftop100 = 0\n",
    "fmean_rank = 0\n",
    "labels = {}\n",
    "preds = {}\n",
    "ranks = {}\n",
    "franks = {}\n",
    "eval_data = test_data\n",
    "n = len(eval_data)\n",
    "for c, r, d in eval_data:\n",
    "    c, r, d = prot_dict[classes[c]], relations[r], prot_dict[classes[d]]\n",
    "    if r not in labels:\n",
    "        labels[r] = np.zeros((len(prot_embeds), len(prot_embeds)), dtype=np.int32)\n",
    "    if r not in preds:\n",
    "        preds[r] = np.zeros((len(prot_embeds), len(prot_embeds)), dtype=np.float32)\n",
    "    labels[r][c, d] = 1\n",
    "    ec = prot_embeds[c, :]\n",
    "    rc = prot_rs[c, :]\n",
    "    er = rembeds[r, :]\n",
    "    ec += er\n",
    "\n",
    "    # Compute similarity\n",
    "    dst = np.linalg.norm(prot_embeds - ec.reshape(1, -1), axis=1)\n",
    "    dst = dst.reshape(-1, 1)\n",
    "    res = np.maximum(0, dst - rc - prot_rs - margin)\n",
    "    res = res.flatten()\n",
    "\n",
    "    preds[r][c, :] = res\n",
    "    index = rankdata(res, method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        top1 += 1\n",
    "    if rank <= 10:\n",
    "        top10 += 1\n",
    "    if rank <= 100:\n",
    "        top100 += 1\n",
    "    mean_rank += rank\n",
    "    if rank not in ranks:\n",
    "        ranks[rank] = 0\n",
    "    ranks[rank] += 1\n",
    "\n",
    "    # Filtered rank\n",
    "    index = rankdata((res * trlabels[r][c, :]), method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        ftop1 += 1\n",
    "    if rank <= 10:\n",
    "        ftop10 += 1\n",
    "    if rank <= 100:\n",
    "        ftop100 += 1\n",
    "    fmean_rank += rank\n",
    "\n",
    "    if rank not in franks:\n",
    "        franks[rank] = 0\n",
    "    franks[rank] += 1\n",
    "top1 /= n\n",
    "top10 /= n\n",
    "top100 /= n\n",
    "mean_rank /= n\n",
    "ftop1 /= n\n",
    "ftop10 /= n\n",
    "ftop100 /= n\n",
    "fmean_rank /= n\n",
    "\n",
    "rank_auc = compute_rank_roc(ranks, len(proteins))\n",
    "frank_auc = compute_rank_roc(franks, len(proteins))\n",
    "\n",
    "print(f'Evaluation for {org_id}')\n",
    "print(f'{top10:.2f} {top100:.2f} {mean_rank:.2f} {rank_auc:.2f}')\n",
    "print(f'{ftop10:.2f} {ftop100:.2f} {fmean_rank:.2f} {frank_auc:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onto_cgans",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
