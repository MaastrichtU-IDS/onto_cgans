{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbf2be2",
   "metadata": {},
   "source": [
    "## Data pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c024d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the whole notebook\n",
    "from xml.etree import ElementTree as ET\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606da578",
   "metadata": {},
   "source": [
    "### Convert XML dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1344a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: clinical signs and symptoms in rare diseases\n",
    "tree = ET.parse('data/en_product4.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "\n",
    "headers = ['HPODisorderSetStatus_id', 'Disorder_id', 'OrphaCode', 'ExpertLink', 'Name', 'DisorderType_id',\n",
    "            'DisorderType_name', 'DisorderGroup_id', 'DisorderGroup_Name', 'HPODisorderAssociation_id',\n",
    "            'HPO_id', 'HPOId', 'HPOTerm', 'HPOFrequency_id', 'HPOFrequency_Name', 'DiagnosticCriteria_id',\n",
    "            'DiagnosticCriteria_Name', 'Source', 'ValidationStatus', 'Online', 'ValidationDate']\n",
    "\n",
    "\n",
    "def find_value(row_data, source_tag, target_tag_name, field, text=True):\n",
    "    \"\"\"Finds a sub-tag of a source tag and inputs its value into a dictionary containing the current row's data\n",
    "    \n",
    "    Args:\n",
    "        row_data (dict):\n",
    "            The data for the current row associated with the csv fields\n",
    "        source_tag (Element):\n",
    "            XML parent tag to search from\n",
    "        target_tag_name (str):\n",
    "            Name of the sub-tag to find\n",
    "        field (str):\n",
    "            Field in the csv file\n",
    "        text (bool):\n",
    "            Indicates if the value of the tag to retrieve is its inner text or its id attribute\n",
    "    Returns:\n",
    "        tag (Element):\n",
    "            Returns the found tag\n",
    "    \"\"\"\n",
    "    tag = source_tag.find(target_tag_name)\n",
    "    tag_v = ''\n",
    "    \n",
    "    if tag is not None:  #retrieving either the inner text or the id attribute of the tag\n",
    "        if text: tag_v = tag.text\n",
    "        elif (len(tag.attrib) > 0): tag_v = tag.attrib['id']\n",
    "    row_data[field] = tag_v if tag_v is not None else ''\n",
    "    \n",
    "    return tag\n",
    "\n",
    "\n",
    "with open('data/en_product4.csv', 'w', encoding='utf-8') as fd:\n",
    "    csvwriter = csv.DictWriter(fd, delimiter=',', fieldnames=headers)\n",
    "    csvwriter.writeheader()\n",
    "    \n",
    "    # iterating through all the disorders\n",
    "    for status in root.find('HPODisorderSetStatusList').findall('HPODisorderSetStatus'):\n",
    "        row_data = {}\n",
    "        row_data['HPODisorderSetStatus_id'] = status.attrib['id']\n",
    "        \n",
    "        disorder_tag = find_value(row_data, status, 'Disorder', 'Disorder_id', text=False)\n",
    "        find_value(row_data, disorder_tag, 'OrphaCode', 'OrphaCode', text=True)\n",
    "        find_value(row_data, disorder_tag, 'ExpertLink', 'ExpertLink', text=True)\n",
    "        find_value(row_data, disorder_tag, 'Name', 'Name', text=True)\n",
    "        \n",
    "        disordertype_tag = find_value(row_data, disorder_tag, 'DisorderType', 'DisorderType_id', text=False)\n",
    "        find_value(row_data, disordertype_tag, 'Name', 'DisorderType_name', text=True)\n",
    "        disordergroup_tag = find_value(row_data, disorder_tag, 'DisorderGroup', 'DisorderGroup_id', text=False)\n",
    "        find_value(row_data, disordergroup_tag, 'Name', 'DisorderGroup_Name', text=True)\n",
    "        \n",
    "        for field in ['Source', 'ValidationStatus', 'Online', 'ValidationDate']:\n",
    "            find_value(row_data, status, field, field, text=True)\n",
    "        \n",
    "        # iterating through all the disorder associations and writing a row for each\n",
    "        for association in disorder_tag.find('HPODisorderAssociationList').findall('HPODisorderAssociation'):\n",
    "            row_data['HPODisorderAssociation_id'] = association.attrib['id']\n",
    "            \n",
    "            hpo_tag = find_value(row_data, association, 'HPO', 'HPO_id', text=False)\n",
    "            find_value(row_data, hpo_tag, 'HPOId', 'HPOId', text=True)\n",
    "            find_value(row_data, hpo_tag, 'HPOTerm', 'HPOTerm', text=True)\n",
    "            hpofrequency_tag = find_value(row_data, association, 'HPOFrequency', 'HPOFrequency_id', text=False)\n",
    "            find_value(row_data, hpofrequency_tag, 'Name', 'HPOFrequency_Name', text=True)\n",
    "            \n",
    "            diagnosticcriteria_tag = find_value(row_data, association, 'DiagnosticCriteria', 'DiagnosticCriteria_id', text=False)\n",
    "            find_value(row_data, diagnosticcriteria_tag, 'Name', 'DiagnosticCriteria_Name', text=True)\n",
    "            \n",
    "            csvwriter.writerow(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebdae01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112241, 21)\n",
      "        HPODisorderSetStatus_id  Disorder_id  OrphaCode  \\\n",
      "789                          51          109        558   \n",
      "790                          51          109        558   \n",
      "791                          51          109        558   \n",
      "792                          51          109        558   \n",
      "794                          51          109        558   \n",
      "...                         ...          ...        ...   \n",
      "111855                     4784        17185     158057   \n",
      "111856                     4784        17185     158057   \n",
      "111857                     4784        17185     158057   \n",
      "111858                     4784        17185     158057   \n",
      "112237                     4807        18531     206599   \n",
      "\n",
      "                                               ExpertLink  \\\n",
      "789     http://www.orpha.net/consor/cgi-bin/OC_Exp.php...   \n",
      "790     http://www.orpha.net/consor/cgi-bin/OC_Exp.php...   \n",
      "791     http://www.orpha.net/consor/cgi-bin/OC_Exp.php...   \n",
      "792     http://www.orpha.net/consor/cgi-bin/OC_Exp.php...   \n",
      "794     http://www.orpha.net/consor/cgi-bin/OC_Exp.php...   \n",
      "...                                                   ...   \n",
      "111855  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...   \n",
      "111856  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...   \n",
      "111857  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...   \n",
      "111858  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...   \n",
      "112237  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...   \n",
      "\n",
      "                                                     Name  DisorderType_id  \\\n",
      "789                                       Marfan syndrome            21394   \n",
      "790                                       Marfan syndrome            21394   \n",
      "791                                       Marfan syndrome            21394   \n",
      "792                                       Marfan syndrome            21394   \n",
      "794                                       Marfan syndrome            21394   \n",
      "...                                                   ...              ...   \n",
      "111855  Acquired hemophagocytic lymphohistiocytosis as...            21429   \n",
      "111856  Acquired hemophagocytic lymphohistiocytosis as...            21429   \n",
      "111857  Acquired hemophagocytic lymphohistiocytosis as...            21429   \n",
      "111858  Acquired hemophagocytic lymphohistiocytosis as...            21429   \n",
      "112237  Isolated asymptomatic elevation of creatine ph...            21408   \n",
      "\n",
      "                                        DisorderType_name  DisorderGroup_id  \\\n",
      "789                                               Disease             36547   \n",
      "790                                               Disease             36547   \n",
      "791                                               Disease             36547   \n",
      "792                                               Disease             36547   \n",
      "794                                               Disease             36547   \n",
      "...                                                   ...               ...   \n",
      "111855  Particular clinical situation in a disease or ...             36547   \n",
      "111856  Particular clinical situation in a disease or ...             36547   \n",
      "111857  Particular clinical situation in a disease or ...             36547   \n",
      "111858  Particular clinical situation in a disease or ...             36547   \n",
      "112237                                 Biological anomaly             36547   \n",
      "\n",
      "       DisorderGroup_Name  HPODisorderAssociation_id  ...       HPOId  \\\n",
      "789              Disorder                     207760  ...  HP:0000768   \n",
      "790              Disorder                     207761  ...  HP:0001065   \n",
      "791              Disorder                     207762  ...  HP:0001166   \n",
      "792              Disorder                     207763  ...  HP:0001519   \n",
      "794              Disorder                     207765  ...  HP:0001763   \n",
      "...                   ...                        ...  ...         ...   \n",
      "111855           Disorder                     525692  ...  HP:0001873   \n",
      "111856           Disorder                     525693  ...  HP:0001875   \n",
      "111857           Disorder                     525694  ...  HP:0001903   \n",
      "111858           Disorder                     525695  ...  HP:0002155   \n",
      "112237           Disorder                     528843  ...  HP:0003236   \n",
      "\n",
      "                               HPOTerm HPOFrequency_id  \\\n",
      "789                   Pectus carinatum           28412   \n",
      "790                   Striae distensae           28412   \n",
      "791                     Arachnodactyly           28412   \n",
      "792      Disproportionate tall stature           28412   \n",
      "794                         Pes planus           28412   \n",
      "...                                ...             ...   \n",
      "111855                Thrombocytopenia           28419   \n",
      "111856                     Neutropenia           28419   \n",
      "111857                          Anemia           28419   \n",
      "111858            Hypertriglyceridemia           28419   \n",
      "112237  Elevated serum creatine kinase           28405   \n",
      "\n",
      "             HPOFrequency_Name DiagnosticCriteria_id  DiagnosticCriteria_Name  \\\n",
      "789     Very frequent (99-80%)               28454.0     Diagnostic criterion   \n",
      "790     Very frequent (99-80%)               28454.0     Diagnostic criterion   \n",
      "791     Very frequent (99-80%)               28454.0     Diagnostic criterion   \n",
      "792     Very frequent (99-80%)               28454.0     Diagnostic criterion   \n",
      "794     Very frequent (99-80%)               28454.0     Diagnostic criterion   \n",
      "...                        ...                   ...                      ...   \n",
      "111855       Frequent (79-30%)               28454.0     Diagnostic criterion   \n",
      "111856       Frequent (79-30%)               28454.0     Diagnostic criterion   \n",
      "111857       Frequent (79-30%)               28454.0     Diagnostic criterion   \n",
      "111858       Frequent (79-30%)               28454.0     Diagnostic criterion   \n",
      "112237         Obligate (100%)               28454.0     Diagnostic criterion   \n",
      "\n",
      "                                                   Source ValidationStatus  \\\n",
      "789                         26621648[PMID]_25323262[PMID]                y   \n",
      "790                         26621648[PMID]_25323262[PMID]                y   \n",
      "791                         26621648[PMID]_25323262[PMID]                y   \n",
      "792                         26621648[PMID]_25323262[PMID]                y   \n",
      "794                         26621648[PMID]_25323262[PMID]                y   \n",
      "...                                                   ...              ...   \n",
      "111855  28938698[PMID]_28648938[PMID]_32195069[PMID]_2...                y   \n",
      "111856  28938698[PMID]_28648938[PMID]_32195069[PMID]_2...                y   \n",
      "111857  28938698[PMID]_28648938[PMID]_32195069[PMID]_2...                y   \n",
      "111858  28938698[PMID]_28648938[PMID]_32195069[PMID]_2...                y   \n",
      "112237                                     26000160[PMID]                y   \n",
      "\n",
      "       Online         ValidationDate  \n",
      "789         y  2016-06-01 00:00:00.0  \n",
      "790         y  2016-06-01 00:00:00.0  \n",
      "791         y  2016-06-01 00:00:00.0  \n",
      "792         y  2016-06-01 00:00:00.0  \n",
      "794         y  2016-06-01 00:00:00.0  \n",
      "...       ...                    ...  \n",
      "111855      y  2021-11-24 00:00:00.0  \n",
      "111856      y  2021-11-24 00:00:00.0  \n",
      "111857      y  2021-11-24 00:00:00.0  \n",
      "111858      y  2021-11-24 00:00:00.0  \n",
      "112237      y  2021-12-07 00:00:00.0  \n",
      "\n",
      "[591 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/en_product4.csv')\n",
    "print(df.shape)\n",
    "print(df[df[\"DiagnosticCriteria_Name\"].notnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36b9de",
   "metadata": {},
   "source": [
    "### Merge ORDO, HP and HOOM ontologies (OWL)\n",
    "\n",
    "https://bioportal.bioontology.org/ontologies/ORDO?p=summary\n",
    "\n",
    "https://bioportal.bioontology.org/ontologies/HP?p=summary\n",
    "\n",
    "https://bioportal.bioontology.org/ontologies/HOOM?p=summary\n",
    "\n",
    "Using Protégé, merge HP into HOOM and ORDO into HOOM\n",
    "\n",
    "'HOOM is a module that qualifies the annotation between a clinical entity and phenotypic abnormalities according to a frequency and by integrating the notion of diagnostic criterion.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04701ac8",
   "metadata": {},
   "source": [
    "### Merge ORDO and HP ontologies using the dataset (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ordo = pd.read_csv('data/ORDO.csv', dtype='object')\n",
    "df_hp = pd.read_csv('data/HP.csv', dtype='object')\n",
    "df_dataset = pd.read_csv('data/en_product4.csv', dtype='object')\n",
    "\n",
    "\n",
    "# prefixes to distinguish the columns from the 2 ontologies\n",
    "df_ordo = df_ordo.add_prefix('ORDO_')\n",
    "df_hp = df_hp.add_prefix('HP_')\n",
    "\n",
    "# normalizing the different columns for the merge\n",
    "df_ordo['OrphaCode'] = df_ordo['ORDO_Class ID'].map(lambda x: x.replace('http://www.orpha.net/ORDO/Orphanet_', ''))\n",
    "df_hp['HPOId'] = df_hp['HP_http://www.w3.org/2004/02/skos/core#notation']\n",
    "df_dataset['OrphaCode'] = df_dataset['OrphaCode'].astype(str)\n",
    "\n",
    "# merge\n",
    "df_merged = pd.merge(df_dataset, df_hp, how='left', on='HPOId')\n",
    "df_merged = pd.merge(df_merged, df_ordo, how='left', on='OrphaCode')\n",
    "\n",
    "df_merged.head(1000).to_csv('data/merged_ontologies.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8362bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv('data/en_product4.csv', dtype='object')\n",
    "df_res = pd.read_csv('data/merged_ontologies.csv', dtype='object')\n",
    "print(df_dataset.shape)\n",
    "print(df_res.shape)\n",
    "print(df_res[['Name', 'HPOId', 'HP_Class ID', 'ORDO_Class ID']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058d15c",
   "metadata": {},
   "source": [
    "### Dataset to triples, entities and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "793735e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_assoc = {  # from csv frequency to frequency code\n",
    "    'Obligate (100%)': 'O',\n",
    "    'Very frequent (99-80%)': 'VF',\n",
    "    'Frequent (79-30%)': 'F',\n",
    "    'Occasional (29-5%)': 'OC',\n",
    "    'Very rare (<4-1%)': 'VR',\n",
    "    'Excluded (0%)': 'E'\n",
    "}\n",
    "\n",
    "freq_code_assoc = {  # from frequency code to output class\n",
    "    'O': 'obligate',\n",
    "    'VF': 'very_frequent',\n",
    "    'F': 'frequent',\n",
    "    'OC': 'occasional',\n",
    "    'VR': 'very_rare',\n",
    "    'E': 'excluded'\n",
    "}\n",
    "\n",
    "dc_association = {  # default: exclusion\n",
    "    'Diagnostic criterion': 'diagnostic_criterion',\n",
    "    'Pathognomonic sign': 'pathognomonic_sign',\n",
    "}\n",
    "\n",
    "\n",
    "def get_association_subclass(orpha, freq, hp):\n",
    "    \"\"\"Returns normalized association class\n",
    "    \n",
    "    Args:\n",
    "        orpha (str):\n",
    "            The prefixed Orphanet code\n",
    "        freq (str):\n",
    "            The frequency text\n",
    "        hp (str):\n",
    "            The prefixed HPO ID\n",
    "    Returns:\n",
    "        (str):\n",
    "            The orphacode, hpo id and frequency association\n",
    "    \"\"\"\n",
    "    return orpha + '_' + hp + '_FREQ:' + freq_assoc.get(freq)\n",
    "\n",
    "\n",
    "def get_association_name(orpha, freq, hp):\n",
    "    \"\"\"Returns textual description of the association class\n",
    "    \n",
    "    Args:\n",
    "        orpha (str):\n",
    "            The prefixed Orphanet code\n",
    "        freq (str):\n",
    "            The frequency text\n",
    "        hp (str):\n",
    "            The prefixed HPO ID\n",
    "    Returns:\n",
    "        (str):\n",
    "            The orphacode, hpo id and frequency association textual_description_with_underscores\n",
    "    \"\"\"\n",
    "    return orpha_entities.get(orpha) + ' and ' + hpo_entities.get(hp) +\\\n",
    "            ' ' + freq_code_assoc.get(freq_assoc.get(freq)) + ' association'\n",
    "\n",
    "\n",
    "def get_normalized_string(s):\n",
    "    \"\"\"Transforms a string to lowercase and replaces all whitespace runs with an underscore\n",
    "    \n",
    "    Args:\n",
    "        s (str):\n",
    "            String to normalize\n",
    "    Returns:\n",
    "        (str):\n",
    "            Normalized string\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s+\", '_', s.lower())\n",
    "\n",
    "\n",
    "df_dataset = pd.read_csv('data/en_product4.csv', dtype='object')\n",
    "df_dataset['OrphaCode'] = df_dataset['OrphaCode'].map(lambda x: 'ORPHA:' + x)\n",
    "\n",
    "# key is id, value is textual_description_with_underscores\n",
    "assoc_entities = {}\n",
    "dc_entities = {'diagnostic_criterion': 'diagnostic_criterion', 'pathognomonic_sign': 'pathognomonic_sign', 'exclusion':'exclusion'}\n",
    "freq_assoc_entities = {'obligate': 'obligate', 'very_frequent': 'very_frequent', 'frequent': 'frequent', \n",
    "                       'occasional': 'occasional', 'very_rare': 'very_rare', 'excluded': 'excluded'}\n",
    "hpo_entities = {}\n",
    "orpha_entities = {}\n",
    "\n",
    "has_object_triples = []  # association has_object HPOId\n",
    "has_subject_triples = []  # association has_subject OrphaCode\n",
    "has_frequency_triples = []  # association has_frequency FrequencyAssociation\n",
    "has_diagnostic_criterion_triples = []  # association has_DC_attribute DC\n",
    "\n",
    "\n",
    "# reading the dataset\n",
    "for orpha, freq, hp, dc, orpha_name, hpo_name in zip(df_dataset['OrphaCode'], df_dataset['HPOFrequency_Name'], \n",
    "                                                     df_dataset['HPOId'], df_dataset['DiagnosticCriteria_Name'], \n",
    "                                                     df_dataset['Name'], df_dataset['HPOTerm']):\n",
    "    if hp not in hpo_entities: hpo_entities[hp] = get_normalized_string(hpo_name)\n",
    "    if orpha not in orpha_entities: orpha_entities[orpha] = get_normalized_string(orpha_name)\n",
    "    ac = get_association_subclass(orpha, freq, hp)\n",
    "    ac_name = get_association_name(orpha, freq, hp)\n",
    "    assoc_entities[ac] = ac_name\n",
    "    has_object_triples.append((ac, 'association_has_object', hp))\n",
    "    has_subject_triples.append((ac, 'association_has_subject', orpha))\n",
    "    if (freq_code_assoc.get(freq_assoc.get(freq)) is None): print('a: ', freq)\n",
    "    has_frequency_triples.append((ac, 'has_frequency', freq_code_assoc.get(freq_assoc.get(freq))))\n",
    "    has_diagnostic_criterion_triples.append((ac, 'has_DC_attribute', dc_association.get(dc, 'exclusion')))\n",
    "\n",
    "    \n",
    "# lists corresponding to each output file\n",
    "triples = []\n",
    "triples_names = []\n",
    "entities = []\n",
    "entities_names = []\n",
    "relations = []\n",
    "\n",
    "# subClassOf triples\n",
    "for k, v in assoc_entities.items(): \n",
    "    triples.append((k, 'subClassOf', 'association'))\n",
    "    triples_names.append((v, 'subClassOf', 'association'))\n",
    "for k, v in dc_entities.items(): \n",
    "    triples.append((k, 'subClassOf', 'diagnostic_criterion'))\n",
    "    triples_names.append((v, 'subClassOf', 'diagnostic_criterion'))\n",
    "for k, v in freq_assoc_entities.items(): \n",
    "    triples.append((k, 'subClassOf', 'frequency_association'))\n",
    "    triples_names.append((v, 'subClassOf', 'frequency_association'))\n",
    "for k, v in hpo_entities.items(): \n",
    "    triples.append((k, 'subClassOf', 'HPO_Id'))\n",
    "    triples_names.append((v, 'subClassOf', 'HPO_Id'))\n",
    "for k, v in orpha_entities.items(): \n",
    "    triples.append((k, 'subClassOf', 'OrphaCode'))\n",
    "    triples_names.append((v, 'subClassOf', 'OrphaCode'))\n",
    "\n",
    "# other properties triples\n",
    "for (s, r, o) in has_object_triples:\n",
    "    triples.append((s, r, o))\n",
    "    triples_names.append((assoc_entities.get(s), r, hpo_entities.get(o)))\n",
    "for (s, r, o) in has_subject_triples:\n",
    "    triples.append((s, r, o))\n",
    "    triples_names.append((assoc_entities.get(s), r, orpha_entities.get(o)))\n",
    "for (s, r, o) in has_frequency_triples:\n",
    "    triples.append((s, r, o))\n",
    "    triples_names.append((assoc_entities.get(s), r, o))\n",
    "for (s, r, o) in has_diagnostic_criterion_triples:\n",
    "    triples.append((s, r, o))\n",
    "    triples_names.append((assoc_entities.get(s), r, o))\n",
    "\n",
    "# entities\n",
    "for i, (k, v) in enumerate({**assoc_entities, **dc_entities, **freq_assoc_entities, **hpo_entities, **orpha_entities}.items()):\n",
    "    entities.append((i, k))\n",
    "    entities_names.append((i, v))\n",
    "\n",
    "# relations\n",
    "for i, r in enumerate(['subClassOf', 'association_has_object', 'association_has_subject', 'has_frequency', 'has_DC_attribute']):\n",
    "    relations.append((i, r))\n",
    "    \n",
    "\n",
    "# writing to the different files\n",
    "with open('data/triples.txt', 'w') as f:\n",
    "    for t in triples:\n",
    "        f.write('\\t'.join(t) + '\\n')\n",
    "with open('data/triples_names.txt', 'w') as f:\n",
    "    for t in triples_names:\n",
    "        f.write('\\t'.join(t) + '\\n')\n",
    "with open('data/entities.dict', 'w') as f:\n",
    "    for t in entities:\n",
    "        f.write('\\t'.join(str(e) for e in t) + '\\n')\n",
    "with open('data/entities_names.dict', 'w') as f:\n",
    "    for t in entities_names:\n",
    "        f.write('\\t'.join(str(e) for e in t) + '\\n')\n",
    "with open('data/relations.dict', 'w') as f:\n",
    "    for t in relations:\n",
    "        f.write('\\t'.join(str(e) for e in t) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3843b5",
   "metadata": {},
   "source": [
    "### Load and split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9165d0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of associations: 112241\n",
      "Number of training associations: 71835\n",
      "Number of validation associations: 17958\n",
      "Number of testing associations: 22448\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/en_product4.csv')\n",
    "\n",
    "df['OrphaCode'] = 'ORPHA:' + df['OrphaCode'].astype(str)\n",
    "df = df[['OrphaCode', 'HPOId']] # only getting RD and HP couples\n",
    "df = df.sample(frac=1, random_state=0).reset_index(drop=True) # seed for reproducibility\n",
    "\n",
    "train_n = int(math.ceil(len(df) * 0.8))\n",
    "valid_n = int(math.ceil(train_n * 0.8))\n",
    "df_train = df.iloc[:valid_n,:]\n",
    "df_valid = df.iloc[valid_n:train_n,:]\n",
    "df_test = df.iloc[train_n:,:]\n",
    "\n",
    "print('Total number of associations:', len(df))\n",
    "print('Number of training associations:', len(df_train))\n",
    "print('Number of validation associations:', len(df_valid))\n",
    "print('Number of testing associations:', len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee650e1",
   "metadata": {},
   "source": [
    "### Save the split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960acaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['train', 'valid', 'test']:\n",
    "    Path(f'data/{folder}/').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_train.to_csv('data/train/associations.txt', header=False, index=False, sep='\\t')\n",
    "df_train.to_csv('data/valid/associations.txt', header=False, index=False, sep='\\t')\n",
    "df_train.to_csv('data/test/associations.txt', header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c74823",
   "metadata": {},
   "source": [
    "### Generate training data classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac556d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/home/leopold/.groovy/grapes/com.google.inject/guice/jars/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)\n",
      "WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "#! groovy el-embeddings/GenerateTrainingDataClasses -on 'data/ordo_hoom_hp.owl' -i 'data/train/associations.txt' -a 'data/train/annotation.txt' -o 'data/train/classes.owl'\n",
    "\n",
    "! groovy el-embeddings/GenerateTrainingDataClasses -on 'data/HOOM_en_1.7.owl' -i 'data/train/associations.txt' -o 'data/train/classes.owl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d391a85",
   "metadata": {},
   "source": [
    "### Normalize training data classes (four normal forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d89add8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/home/leopold/Documents/4A/stage/onto_cgans/el-embeddings/jar/jcel.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)\n",
      "WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Caught: de.tudresden.inf.lat.jcel.owlapi.translator.TranslationException: The translation map is incomplete. Item id was not found: '2017-06-14T00:00:00.0'.\n",
      "de.tudresden.inf.lat.jcel.owlapi.translator.TranslationException: The translation map is incomplete. Item id was not found: '2017-06-14T00:00:00.0'.\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.TranslationException.newIncompleteMapException(TranslationException.java:67)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.TranslationRepository.getId(TranslationRepository.java:318)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.ClassExpressionTranslator.visit(ClassExpressionTranslator.java:156)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.ClassExpressionTranslator.visit(ClassExpressionTranslator.java:87)\n",
      "\tat uk.ac.manchester.cs.owl.owlapi.OWLDataHasValueImpl.accept(OWLDataHasValueImpl.java:137)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.ClassExpressionTranslator.lambda$visit$0(ClassExpressionTranslator.java:214)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.ClassExpressionTranslator.visit(ClassExpressionTranslator.java:213)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.ClassExpressionTranslator.visit(ClassExpressionTranslator.java:87)\n",
      "\tat uk.ac.manchester.cs.owl.owlapi.OWLObjectIntersectionOfImpl.accept(OWLObjectIntersectionOfImpl.java:86)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.AxiomTranslator.translateClassExpression(AxiomTranslator.java:177)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.AxiomTranslator.lambda$visit$3(AxiomTranslator.java:388)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.AxiomTranslator.visit(AxiomTranslator.java:388)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.AxiomTranslator.visit(AxiomTranslator.java:126)\n",
      "\tat uk.ac.manchester.cs.owl.owlapi.OWLEquivalentClassesAxiomImpl.accept(OWLEquivalentClassesAxiomImpl.java:176)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.Translator.lambda$translateSA$0(Translator.java:164)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.translator.Translator.translateSA(Translator.java:163)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.main.JcelReasoner.getIntegerOntology(JcelReasoner.java:354)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.main.JcelReasoner.resetReasoner(JcelReasoner.java:724)\n",
      "\tat de.tudresden.inf.lat.jcel.owlapi.main.JcelReasoner.<init>(JcelReasoner.java:137)\n",
      "\tat Normalizer.run(Normalizer.groovy:65)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n"
     ]
    }
   ],
   "source": [
    "#! groovy -cp el-embeddings/jar/jcel.jar el-embeddings/Normalizer.groovy -i 'data/ordo_hoom_hp.owl' -o 'data/train/classes-normalized.owl'\n",
    "\n",
    "! groovy -cp el-embeddings/jar/jcel.jar el-embeddings/Normalizer.groovy -i 'data/HOOM_en_1.7.owl' -o 'data/train/classes-normalized.owl'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onto_cgans",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
