{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbf2be2",
   "metadata": {},
   "source": [
    "# Data pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4f089-bb5f-4fa5-a16a-0da82c83c362",
   "metadata": {},
   "source": [
    "### Imports and paths initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c024d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the whole notebook\n",
    "from xml.etree import ElementTree as ET\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Data pre-process',\n",
    "        usage=''\n",
    "    )\n",
    "    parser.add_argument('--data_dir', type=str, default='data')\n",
    "    parser.add_argument('--onto_dir', type=str, default='ontology')\n",
    "    parser.add_argument('--syn_data_dir', type=str, default='syn_data')\n",
    "\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "# set the path to your data folders here\n",
    "args = parse_args(args=['--data_dir', '../persistent/data'])\n",
    "onto_dir_path = os.path.join(args.data_dir, args.onto_dir)\n",
    "syn_data_dir_path = os.path.join(args.data_dir, args.syn_data_dir)\n",
    "\n",
    "if not os.path.exists(onto_dir_path):\n",
    "    raise ValueError(f'You need an existing ontology directory with the \\\n",
    "                     XML dataset inside it, please create \\'{onto_dir_path}\\'')\n",
    "\n",
    "if not os.path.exists(syn_data_dir_path):\n",
    "    os.makedirs(syn_data_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606da578",
   "metadata": {},
   "source": [
    "### Convert XML dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1344a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: clinical signs and symptoms in rare diseases\n",
    "# http://www.orphadata.org/cgi-bin/index.php (Phenotypes associated with rare disorders)\n",
    "\n",
    "\n",
    "tree = ET.parse(os.path.join(onto_dir_path, 'en_product4.xml'))\n",
    "root = tree.getroot()\n",
    "\n",
    "\n",
    "headers = ['HPODisorderSetStatus_id', 'Disorder_id', 'OrphaCode', 'ExpertLink',\n",
    "           'Name', 'DisorderType_id', 'DisorderType_name', 'DisorderGroup_id',\n",
    "           'DisorderGroup_Name', 'HPODisorderAssociation_id', 'HPO_id',\n",
    "           'HPOId', 'HPOTerm', 'HPOFrequency_id', 'HPOFrequency_Name',\n",
    "           'DiagnosticCriteria_id', 'DiagnosticCriteria_Name', 'Source',\n",
    "           'ValidationStatus', 'Online', 'ValidationDate']\n",
    "\n",
    "\n",
    "def find_value(row_data, source_tag, target_tag_name, field, text=True):\n",
    "    \"\"\"Finds a sub-tag of a source tag and inputs its value into a dictionary\n",
    "        containing the current row's data\n",
    "\n",
    "    Args:\n",
    "        row_data (dict):\n",
    "            The data for the current row associated with the csv fields\n",
    "        source_tag (Element):\n",
    "            XML parent tag to search from\n",
    "        target_tag_name (str):\n",
    "            Name of the sub-tag to find\n",
    "        field (str):\n",
    "            Field in the csv file\n",
    "        text (bool):\n",
    "            Indicates if the value of the tag to retrieve is its inner text \n",
    "            or its id attribute\n",
    "    Returns:\n",
    "        tag (Element):\n",
    "            Returns the found tag\n",
    "    \"\"\"\n",
    "    tag = source_tag.find(target_tag_name)\n",
    "    tag_v = ''\n",
    "\n",
    "    if tag is not None:  # retrieving either the inner text or the id attribute of the tag\n",
    "        if text:\n",
    "            tag_v = tag.text\n",
    "        elif (len(tag.attrib) > 0):\n",
    "            tag_v = tag.attrib['id']\n",
    "    row_data[field] = tag_v if tag_v is not None else ''\n",
    "\n",
    "    return tag\n",
    "\n",
    "\n",
    "with open(os.path.join(onto_dir_path, 'en_product4.csv'), 'w', encoding='utf-8') as fd:\n",
    "    csvwriter = csv.DictWriter(fd, delimiter=',', fieldnames=headers)\n",
    "    csvwriter.writeheader()\n",
    "\n",
    "    # iterating through all the disorders\n",
    "    for status in root.find('HPODisorderSetStatusList').findall('HPODisorderSetStatus'):\n",
    "        row_data = {}\n",
    "        row_data['HPODisorderSetStatus_id'] = status.attrib['id']\n",
    "\n",
    "        disorder_tag = find_value(row_data, status, 'Disorder', 'Disorder_id', text=False)\n",
    "        find_value(row_data, disorder_tag, 'OrphaCode', 'OrphaCode', text=True)\n",
    "        find_value(row_data, disorder_tag, 'ExpertLink', 'ExpertLink', text=True)\n",
    "        find_value(row_data, disorder_tag, 'Name', 'Name', text=True)\n",
    "\n",
    "        disordertype_tag = find_value(row_data, disorder_tag, 'DisorderType', 'DisorderType_id', text=False)\n",
    "        find_value(row_data, disordertype_tag, 'Name', 'DisorderType_name', text=True)\n",
    "        disordergroup_tag = find_value(row_data, disorder_tag, 'DisorderGroup', 'DisorderGroup_id', text=False)\n",
    "        find_value(row_data, disordergroup_tag, 'Name', 'DisorderGroup_Name', text=True)\n",
    "\n",
    "        for field in ['Source', 'ValidationStatus', 'Online', 'ValidationDate']:\n",
    "            find_value(row_data, status, field, field, text=True)\n",
    "\n",
    "        # iterating through all the disorder associations and writing a row for each association\n",
    "        for association in disorder_tag.find('HPODisorderAssociationList').findall('HPODisorderAssociation'):\n",
    "            row_data['HPODisorderAssociation_id'] = association.attrib['id']\n",
    "\n",
    "            hpo_tag = find_value(row_data, association, 'HPO', 'HPO_id', text=False)\n",
    "            find_value(row_data, hpo_tag, 'HPOId', 'HPOId', text=True)\n",
    "            find_value(row_data, hpo_tag, 'HPOTerm', 'HPOTerm', text=True)\n",
    "            hpofrequency_tag = find_value(row_data, association, 'HPOFrequency', 'HPOFrequency_id', text=False)\n",
    "            find_value(row_data, hpofrequency_tag, 'Name', 'HPOFrequency_Name', text=True)\n",
    "\n",
    "            diagnosticcriteria_tag = find_value(row_data, association, 'DiagnosticCriteria', 'DiagnosticCriteria_id', text=False)\n",
    "            find_value(row_data, diagnosticcriteria_tag, 'Name', 'DiagnosticCriteria_Name', text=True)\n",
    "\n",
    "            csvwriter.writerow(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebdae01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HPODisorderSetStatus_id</th>\n",
       "      <th>Disorder_id</th>\n",
       "      <th>OrphaCode</th>\n",
       "      <th>ExpertLink</th>\n",
       "      <th>Name</th>\n",
       "      <th>DisorderType_id</th>\n",
       "      <th>DisorderType_name</th>\n",
       "      <th>DisorderGroup_id</th>\n",
       "      <th>DisorderGroup_Name</th>\n",
       "      <th>HPODisorderAssociation_id</th>\n",
       "      <th>...</th>\n",
       "      <th>HPOId</th>\n",
       "      <th>HPOTerm</th>\n",
       "      <th>HPOFrequency_id</th>\n",
       "      <th>HPOFrequency_Name</th>\n",
       "      <th>DiagnosticCriteria_id</th>\n",
       "      <th>DiagnosticCriteria_Name</th>\n",
       "      <th>Source</th>\n",
       "      <th>ValidationStatus</th>\n",
       "      <th>Online</th>\n",
       "      <th>ValidationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327485</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0000256</td>\n",
       "      <td>Macrocephaly</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327486</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001249</td>\n",
       "      <td>Intellectual disability</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327487</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001250</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327488</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001257</td>\n",
       "      <td>Spasticity</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327489</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001274</td>\n",
       "      <td>Agenesis of corpus callosum</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HPODisorderSetStatus_id  Disorder_id  OrphaCode  \\\n",
       "0                        1            2         58   \n",
       "1                        1            2         58   \n",
       "2                        1            2         58   \n",
       "3                        1            2         58   \n",
       "4                        1            2         58   \n",
       "\n",
       "                                          ExpertLink               Name  \\\n",
       "0  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "1  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "2  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "3  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "4  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "\n",
       "   DisorderType_id DisorderType_name  DisorderGroup_id DisorderGroup_Name  \\\n",
       "0            21394           Disease             36547           Disorder   \n",
       "1            21394           Disease             36547           Disorder   \n",
       "2            21394           Disease             36547           Disorder   \n",
       "3            21394           Disease             36547           Disorder   \n",
       "4            21394           Disease             36547           Disorder   \n",
       "\n",
       "   HPODisorderAssociation_id  ...       HPOId                      HPOTerm  \\\n",
       "0                     327485  ...  HP:0000256                 Macrocephaly   \n",
       "1                     327486  ...  HP:0001249      Intellectual disability   \n",
       "2                     327487  ...  HP:0001250                     Seizures   \n",
       "3                     327488  ...  HP:0001257                   Spasticity   \n",
       "4                     327489  ...  HP:0001274  Agenesis of corpus callosum   \n",
       "\n",
       "  HPOFrequency_id       HPOFrequency_Name DiagnosticCriteria_id  \\\n",
       "0           28412  Very frequent (99-80%)                   NaN   \n",
       "1           28412  Very frequent (99-80%)                   NaN   \n",
       "2           28412  Very frequent (99-80%)                   NaN   \n",
       "3           28412  Very frequent (99-80%)                   NaN   \n",
       "4           28412  Very frequent (99-80%)                   NaN   \n",
       "\n",
       "   DiagnosticCriteria_Name Source ValidationStatus Online  \\\n",
       "0                      NaN    NaN                y      y   \n",
       "1                      NaN    NaN                y      y   \n",
       "2                      NaN    NaN                y      y   \n",
       "3                      NaN    NaN                y      y   \n",
       "4                      NaN    NaN                y      y   \n",
       "\n",
       "          ValidationDate  \n",
       "0  2016-06-01 00:00:00.0  \n",
       "1  2016-06-01 00:00:00.0  \n",
       "2  2016-06-01 00:00:00.0  \n",
       "3  2016-06-01 00:00:00.0  \n",
       "4  2016-06-01 00:00:00.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058d15c",
   "metadata": {},
   "source": [
    "### Dataset to triples, entities and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "793735e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_assoc = {  # from csv frequency to frequency code\n",
    "    'Obligate (100%)': 'O',\n",
    "    'Very frequent (99-80%)': 'VF',\n",
    "    'Frequent (79-30%)': 'F',\n",
    "    'Occasional (29-5%)': 'OC',\n",
    "    'Very rare (<4-1%)': 'VR',\n",
    "    'Excluded (0%)': 'E'\n",
    "}\n",
    "\n",
    "freq_code_assoc = {  # from frequency code to output class\n",
    "    'O': 'obligate',\n",
    "    'VF': 'very_frequent',\n",
    "    'F': 'frequent',\n",
    "    'OC': 'occasional',\n",
    "    'VR': 'very_rare',\n",
    "    'E': 'excluded'\n",
    "}\n",
    "\n",
    "dc_association = {  # default: exclusion\n",
    "    'Diagnostic criterion': 'diagnostic_criterion',\n",
    "    'Pathognomonic sign': 'pathognomonic_sign',\n",
    "}\n",
    "\n",
    "\n",
    "def get_association_subclass(orpha, freq, hp):\n",
    "    \"\"\"Returns normalized association class\n",
    "\n",
    "    Args:\n",
    "        orpha (str):\n",
    "            The prefixed Orphanet code\n",
    "        freq (str):\n",
    "            The frequency text\n",
    "        hp (str):\n",
    "            The prefixed HPO ID\n",
    "    Returns:\n",
    "        (str):\n",
    "            The orphacode, hpo id and frequency association\n",
    "    \"\"\"\n",
    "    return orpha + '_' + hp + '_FREQ:' + freq_assoc.get(freq)\n",
    "\n",
    "\n",
    "def get_association_name(orpha, freq, hp):\n",
    "    \"\"\"Returns textual description of the association class\n",
    "\n",
    "    Args:\n",
    "        orpha (str):\n",
    "            The prefixed Orphanet code\n",
    "        freq (str):\n",
    "            The frequency text\n",
    "        hp (str):\n",
    "            The prefixed HPO ID\n",
    "    Returns:\n",
    "        (str):\n",
    "            The orphacode, hpo id and frequency association \n",
    "            textual_description_with_underscores\n",
    "    \"\"\"\n",
    "    return get_normalized_string(orpha_entities.get(orpha) + ' and ' +\n",
    "                                 hpo_entities.get(hp) + ' ' +\n",
    "                                 freq_code_assoc.get(freq_assoc.get(freq)) +\n",
    "                                 ' association')\n",
    "\n",
    "\n",
    "def get_normalized_string(s):\n",
    "    \"\"\"Transforms a string to lowercase and replaces all whitespace runs with an underscore\n",
    "\n",
    "    Args:\n",
    "        s (str):\n",
    "            String to normalize\n",
    "    Returns:\n",
    "        (str):\n",
    "            The normalized string\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s+\", '_', s.lower())\n",
    "\n",
    "\n",
    "df_dataset = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'), \n",
    "                         dtype='object')\n",
    "df_dataset['OrphaCode'] = df_dataset['OrphaCode'].map(lambda x: 'ORPHA:' + x)\n",
    "\n",
    "# key is id, value is textual_description_with_underscores\n",
    "assoc_entities = {}\n",
    "dc_entities = {'diagnostic_criterion': 'diagnostic_criterion',\n",
    "               'pathognomonic_sign': 'pathognomonic_sign',\n",
    "               'exclusion': 'exclusion'}\n",
    "freq_assoc_entities = {'obligate': 'obligate', 'very_frequent': 'very_frequent',\n",
    "                       'frequent': 'frequent', 'occasional': 'occasional',\n",
    "                       'very_rare': 'very_rare', 'excluded': 'excluded'}\n",
    "hpo_entities = {}\n",
    "orpha_entities = {}\n",
    "\n",
    "has_object_triples = []  # association has_object HPOId\n",
    "has_subject_triples = []  # association has_subject OrphaCode\n",
    "has_frequency_triples = []  # association has_frequency FrequencyAssociation\n",
    "has_diagnostic_criterion_triples = []  # association has_DC_attribute DC\n",
    "\n",
    "\n",
    "# reading the dataset\n",
    "for orpha, freq, hp, dc, \\\n",
    "    orpha_name, hpo_name in zip(df_dataset['OrphaCode'],\n",
    "                                df_dataset['HPOFrequency_Name'],\n",
    "                                df_dataset['HPOId'],\n",
    "                                df_dataset['DiagnosticCriteria_Name'],\n",
    "                                df_dataset['Name'],\n",
    "                                df_dataset['HPOTerm']):\n",
    "    if hp not in hpo_entities:\n",
    "        hpo_entities[hp] = get_normalized_string(hpo_name)\n",
    "    if orpha not in orpha_entities:\n",
    "        orpha_entities[orpha] = get_normalized_string(orpha_name)\n",
    "\n",
    "    ac = get_association_subclass(orpha, freq, hp)\n",
    "    ac_name = get_association_name(orpha, freq, hp)\n",
    "    assoc_entities[ac] = ac_name\n",
    "\n",
    "    has_object_triples.append((ac, 'association_has_object', hp))\n",
    "    has_subject_triples.append((ac, 'association_has_subject', orpha))\n",
    "    has_frequency_triples.append((ac, 'has_frequency', freq_code_assoc.get(freq_assoc.get(freq))))\n",
    "    has_diagnostic_criterion_triples.append((ac, 'has_DC_attribute', dc_association.get(dc, 'exclusion')))\n",
    "\n",
    "\n",
    "# lists corresponding to each output file\n",
    "triples = []\n",
    "triples_names = []\n",
    "entities = []\n",
    "entities_names = []\n",
    "relations = []\n",
    "\n",
    "# subClassOf triples\n",
    "entities_and_parent_class = [\n",
    "    (assoc_entities, 'association'),\n",
    "    (dc_entities, 'diagnostic_criteria'), \n",
    "    (freq_assoc_entities, 'frequency_association'), \n",
    "    (hpo_entities, 'HPO_Id'), \n",
    "    (orpha_entities, 'OrphaCode')\n",
    "]\n",
    "for (entities_dict, parent_class) in entities_and_parent_class:\n",
    "    for k, v in entities_dict.items(): \n",
    "        triples.append((k, 'subClassOf', get_normalized_string(parent_class)))\n",
    "        triples_names.append((v, 'subClassOf', parent_class))\n",
    "\n",
    "# other properties triples\n",
    "triples_and_entities = [\n",
    "    (has_object_triples, hpo_entities), \n",
    "    (has_subject_triples, orpha_entities), \n",
    "    (has_frequency_triples, freq_assoc_entities), \n",
    "    (has_diagnostic_criterion_triples, dc_entities)\n",
    "]\n",
    "for (properties_triples, entities_dict) in triples_and_entities:\n",
    "    for (s, r, o) in properties_triples:\n",
    "        triples.append((s, r, o))\n",
    "        triples_names.append((assoc_entities.get(s), r, entities_dict.get(o)))\n",
    "\n",
    "# parent entities\n",
    "for i, (k, v) in enumerate(entities_and_parent_class):\n",
    "    parent = get_normalized_string(v)\n",
    "    entities.append((i, parent))\n",
    "    entities_names.append((i, parent))\n",
    "# entities\n",
    "parents_count = len(entities)\n",
    "for i, (k, v) in enumerate({**assoc_entities, **dc_entities, **freq_assoc_entities,\n",
    "                            **hpo_entities, **orpha_entities}.items()):\n",
    "    entities.append((i+parents_count, k))\n",
    "    entities_names.append((i+parents_count, v))\n",
    "\n",
    "# relations\n",
    "for i, r in enumerate(['subClassOf', 'association_has_object',\n",
    "                       'association_has_subject', 'has_frequency', \n",
    "                       'has_DC_attribute']):\n",
    "    relations.append((i, r))\n",
    "\n",
    "\n",
    "# writing to the different files\n",
    "lists_and_files = [\n",
    "    (triples, 'triples.txt'), \n",
    "    (triples_names, 'triples_names.txt'),\n",
    "    (entities, 'entities.dict'),\n",
    "    (entities_names, 'entities_names.dict'),\n",
    "    (relations, 'relations.dict')\n",
    "]\n",
    "for (l, n) in lists_and_files:\n",
    "    with open(os.path.join(onto_dir_path, n), 'w') as f:\n",
    "        for t in l:\n",
    "            f.write('\\t'.join(str(e) for e in t) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70178f4",
   "metadata": {},
   "source": [
    "### Merge ORDO and HP ontologies using the dataset (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16c0a8c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../persistent/data/ontology/ORDO.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8494/2587233701.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_ordo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monto_dir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ORDO.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_hp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monto_dir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HP.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monto_dir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'en_product4.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    648\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../persistent/data/ontology/ORDO.csv'"
     ]
    }
   ],
   "source": [
    "df_ordo = pd.read_csv(os.path.join(onto_dir_path, 'ORDO.csv'), dtype='object')\n",
    "df_hp = pd.read_csv(os.path.join(onto_dir_path, 'HP.csv'), dtype='object')\n",
    "df_dataset = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'), dtype='object')\n",
    "\n",
    "\n",
    "# prefixes to distinguish the columns from the 2 ontologies\n",
    "df_ordo = df_ordo.add_prefix('ORDO_')\n",
    "df_hp = df_hp.add_prefix('HP_')\n",
    "\n",
    "# normalizing the different columns for the merge\n",
    "df_ordo['OrphaCode'] = df_ordo['ORDO_Class ID'].map(lambda x: x.replace('http://www.orpha.net/ORDO/Orphanet_', ''))\n",
    "df_hp['HPOId'] = df_hp['HP_http://www.w3.org/2004/02/skos/core#notation']\n",
    "df_dataset['OrphaCode'] = df_dataset['OrphaCode'].astype(str)\n",
    "\n",
    "# merge\n",
    "df_merged = pd.merge(df_dataset, df_hp, how='left', on='HPOId')\n",
    "df_merged = pd.merge(df_merged, df_ordo, how='left', on='OrphaCode')\n",
    "\n",
    "df_merged.head(1000).to_csv(os.path.join(onto_dir_path, 'merged_ontologies.csv'), encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f95eb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HPODisorderSetStatus_id</th>\n",
       "      <th>Disorder_id</th>\n",
       "      <th>OrphaCode</th>\n",
       "      <th>ExpertLink</th>\n",
       "      <th>Name</th>\n",
       "      <th>DisorderType_id</th>\n",
       "      <th>DisorderType_name</th>\n",
       "      <th>DisorderGroup_id</th>\n",
       "      <th>DisorderGroup_Name</th>\n",
       "      <th>HPODisorderAssociation_id</th>\n",
       "      <th>...</th>\n",
       "      <th>ORDO_http://www.w3.org/2004/02/skos/core#notation</th>\n",
       "      <th>ORDO_https://creativecommons.org/licenses/permits</th>\n",
       "      <th>ORDO_https://creativecommons.org/licenses/requires</th>\n",
       "      <th>ORDO_major susceptibility factor in</th>\n",
       "      <th>ORDO_manual_assertion</th>\n",
       "      <th>ORDO_modifying germline mutation in</th>\n",
       "      <th>ORDO_part of a fusion gene in</th>\n",
       "      <th>ORDO_part_of</th>\n",
       "      <th>ORDO_present_in</th>\n",
       "      <th>ORDO_role in the phenotype of</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327485</td>\n",
       "      <td>...</td>\n",
       "      <td>ORPHA:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_182070|http...</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_410102|http...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327486</td>\n",
       "      <td>...</td>\n",
       "      <td>ORPHA:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_182070|http...</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_410102|http...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327487</td>\n",
       "      <td>...</td>\n",
       "      <td>ORPHA:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_182070|http...</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_410102|http...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327488</td>\n",
       "      <td>...</td>\n",
       "      <td>ORPHA:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_182070|http...</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_410102|http...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327489</td>\n",
       "      <td>...</td>\n",
       "      <td>ORPHA:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_182070|http...</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_410102|http...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  HPODisorderSetStatus_id Disorder_id OrphaCode  \\\n",
       "0                       1           2        58   \n",
       "1                       1           2        58   \n",
       "2                       1           2        58   \n",
       "3                       1           2        58   \n",
       "4                       1           2        58   \n",
       "\n",
       "                                          ExpertLink               Name  \\\n",
       "0  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "1  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "2  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "3  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "4  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "\n",
       "  DisorderType_id DisorderType_name DisorderGroup_id DisorderGroup_Name  \\\n",
       "0           21394           Disease            36547           Disorder   \n",
       "1           21394           Disease            36547           Disorder   \n",
       "2           21394           Disease            36547           Disorder   \n",
       "3           21394           Disease            36547           Disorder   \n",
       "4           21394           Disease            36547           Disorder   \n",
       "\n",
       "  HPODisorderAssociation_id  ...  \\\n",
       "0                    327485  ...   \n",
       "1                    327486  ...   \n",
       "2                    327487  ...   \n",
       "3                    327488  ...   \n",
       "4                    327489  ...   \n",
       "\n",
       "  ORDO_http://www.w3.org/2004/02/skos/core#notation  \\\n",
       "0                                          ORPHA:58   \n",
       "1                                          ORPHA:58   \n",
       "2                                          ORPHA:58   \n",
       "3                                          ORPHA:58   \n",
       "4                                          ORPHA:58   \n",
       "\n",
       "  ORDO_https://creativecommons.org/licenses/permits  \\\n",
       "0                                               NaN   \n",
       "1                                               NaN   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               NaN   \n",
       "\n",
       "  ORDO_https://creativecommons.org/licenses/requires  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "  ORDO_major susceptibility factor in ORDO_manual_assertion  \\\n",
       "0                                 NaN                   NaN   \n",
       "1                                 NaN                   NaN   \n",
       "2                                 NaN                   NaN   \n",
       "3                                 NaN                   NaN   \n",
       "4                                 NaN                   NaN   \n",
       "\n",
       "  ORDO_modifying germline mutation in ORDO_part of a fusion gene in  \\\n",
       "0                                 NaN                           NaN   \n",
       "1                                 NaN                           NaN   \n",
       "2                                 NaN                           NaN   \n",
       "3                                 NaN                           NaN   \n",
       "4                                 NaN                           NaN   \n",
       "\n",
       "                                        ORDO_part_of  \\\n",
       "0  http://www.orpha.net/ORDO/Orphanet_182070|http...   \n",
       "1  http://www.orpha.net/ORDO/Orphanet_182070|http...   \n",
       "2  http://www.orpha.net/ORDO/Orphanet_182070|http...   \n",
       "3  http://www.orpha.net/ORDO/Orphanet_182070|http...   \n",
       "4  http://www.orpha.net/ORDO/Orphanet_182070|http...   \n",
       "\n",
       "                                     ORDO_present_in  \\\n",
       "0  http://www.orpha.net/ORDO/Orphanet_410102|http...   \n",
       "1  http://www.orpha.net/ORDO/Orphanet_410102|http...   \n",
       "2  http://www.orpha.net/ORDO/Orphanet_410102|http...   \n",
       "3  http://www.orpha.net/ORDO/Orphanet_410102|http...   \n",
       "4  http://www.orpha.net/ORDO/Orphanet_410102|http...   \n",
       "\n",
       "  ORDO_role in the phenotype of  \n",
       "0                           NaN  \n",
       "1                           NaN  \n",
       "2                           NaN  \n",
       "3                           NaN  \n",
       "4                           NaN  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'), dtype='object')\n",
    "df_res = pd.read_csv(os.path.join(onto_dir_path, 'merged_ontologies.csv'), dtype='object')\n",
    "\n",
    "df_res.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d9077-ef7a-45c5-b5f3-c6361b8a9000",
   "metadata": {},
   "source": [
    "### Synthetic data generation from the ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06240382-5159-4e7d-8e3e-e35997620c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RDs: 4243 seen RDs: 3394, unseen RDs: 849\n",
      "Generating smaller data files for debugging\n",
      "Number of lines in the small datasets: 50\n"
     ]
    }
   ],
   "source": [
    "frequency_dict = {  # frequency ids + associated probability\n",
    "    28405: 1,  # Obligate (100%)\n",
    "    28412: 0.895,  # Very frequent (99-80%)\n",
    "    28419: 0.545,  # Frequent (79-30%)\n",
    "    28426: 0.17,  # Occasional (29-5%)\n",
    "    28433: 0.025,  # Very rare (<4-1%)\n",
    "    28440: 0  # Excluded (0%)\n",
    "}\n",
    "\n",
    "\n",
    "def get_normalized_string(s):\n",
    "    \"\"\"Transforms a string to lowercase and replaces all whitespace runs with an underscore\n",
    "\n",
    "    Args:\n",
    "        s (str):\n",
    "            String to normalize\n",
    "    Returns:\n",
    "        (str):\n",
    "            The normalized string\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s+\", '_', s.lower())\n",
    "\n",
    "\n",
    "def gen_syn_data(patients_per_rd=10, max_nb_hp= 20, unseen_pct=0.2, gen_small_file=True):\n",
    "    \"\"\"Generates synthetic seen and unseen data from the ontology into files\n",
    "\n",
    "    Args:\n",
    "        patients_per_rd (int):\n",
    "            Number of generated patients per RD\n",
    "        names (bool):\n",
    "            Whether the files listing the names of RDs and HPs instead of their codes/ids should be generated too\n",
    "        unseen_pct (float):\n",
    "            The percentage of RDs to keep for unseen patients samples\n",
    "        gen_small_file (bool):\n",
    "            Whether to generate very small files to debug the model or not\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'))\n",
    "\n",
    "    # randomizing the order of the RDs for randomized seen/unseen RDs\n",
    "    df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "    grouped = df_shuffled.groupby('OrphaCode', sort=False)\n",
    "\n",
    "    patients_count = 0\n",
    "    seen_patients_data = [['patient_id', 'rare_disease']]\n",
    "    seen_patients_data[0] += [(f'phenotype_{i+1}') for i in range(max_nb_hp)]\n",
    "    unseen_patients_data = [['patient_id']]\n",
    "    unseen_patients_data[0] += [(f'phenotype_{i+1}') for i in range(max_nb_hp)]\n",
    "    lists_n_files = [\n",
    "        (seen_patients_data, 'syn_patients_data_seen.csv'),\n",
    "        (unseen_patients_data, 'syn_patients_data_unseen.csv')\n",
    "    ]\n",
    "\n",
    "    rd_count = grouped.ngroups\n",
    "    seen_unseen_th = rd_count - math.ceil(rd_count*unseen_pct)  # threshold of RDs to switch to unseen RDs\n",
    "\n",
    "    for group_nb, (name, group) in enumerate(grouped):  # for each RD\n",
    "        hp_count = len(group)\n",
    "        # generate patients_per_rd patients for each RD\n",
    "        for patient_id in range(patients_count, patients_count+patients_per_rd):\n",
    "            temp_hp = []\n",
    "            proba_results = np.random.rand(hp_count)  # generating random floats for probabilities\n",
    "            rd = ''\n",
    "            for i, (rd_name, hp_name, frequency_id) in enumerate(zip(\n",
    "                                                            group['Name'],\n",
    "                                                            group['HPOTerm'],\n",
    "                                                            group['HPOFrequency_id'])):\n",
    "                if (proba_results[i] >= 1 - frequency_dict[frequency_id]):  # comparing generated float and proba\n",
    "                    temp_hp.append(get_normalized_string(hp_name))\n",
    "                    if rd == '':\n",
    "                        rd = get_normalized_string(rd_name)\n",
    "            if len(temp_hp) > max_nb_hp:\n",
    "                random.shuffle(temp_hp)\n",
    "                temp_hp = temp_hp[:max_nb_hp]\n",
    "            if (group_nb <= seen_unseen_th):\n",
    "                seen_patients_data.append([patient_id, rd] + temp_hp)\n",
    "            else:\n",
    "                unseen_patients_data.append([patient_id, rd] + temp_hp)\n",
    "\n",
    "        patients_count += patients_per_rd\n",
    "\n",
    "    # writing the 2 files\n",
    "    for lst, fn in lists_n_files:\n",
    "        if (len(lst) > 0):\n",
    "            pd.DataFrame(lst).to_csv(\n",
    "                os.path.join(syn_data_dir_path, fn),\n",
    "                encoding='utf-8', index=False, header=False\n",
    "            )\n",
    "\n",
    "    print(f'Total RDs: {rd_count} seen RDs: {seen_unseen_th}, unseen RDs: {rd_count-seen_unseen_th}')\n",
    "\n",
    "    if gen_small_file:\n",
    "        NUMBER_OF_DISEASES = 5\n",
    "        print('Generating smaller data files for debugging')\n",
    "        for lst, fn in lists_n_files:\n",
    "            pd.DataFrame(lst[:NUMBER_OF_DISEASES*patients_per_rd]).to_csv(\n",
    "                os.path.join(syn_data_dir_path, 'small_' + fn),\n",
    "                encoding='utf-8', index=False, header=False\n",
    "            )\n",
    "        print(f'Number of lines in the small datasets: {NUMBER_OF_DISEASES*patients_per_rd}')\n",
    "\n",
    "\n",
    "gen_syn_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac8a38-b08f-4db8-b91b-1fdb3034ad64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afdfde1c",
   "metadata": {},
   "source": [
    "### OrphaCode and HPO IDs dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a01ce23c-0768-4088-bafe-c6b58d96bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'))\n",
    "\n",
    "df_hp = df[['HPOId', 'HPOTerm']].drop_duplicates()\n",
    "df_rd = df[['OrphaCode', 'Name']].drop_duplicates()\n",
    "df_rd['OrphaCode'] = 'ORPHA:' + df_rd['OrphaCode'].astype(str)\n",
    "\n",
    "df_hp.to_csv(os.path.join(onto_dir_path, 'hp.dict'), encoding='utf-8', index=False, header=False)\n",
    "df_rd.to_csv(os.path.join(onto_dir_path, 'rd.dict'), encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4a7fc9-4b06-48bb-a81b-11c5e3b2a5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.480171015774731\n",
      "12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_names = [\"patient_id\", \"rare_disease\", \"phenotype\"]\n",
    "tabular_data = pd.read_csv(\"../persistent/data/syn_data/syn_patients_data_seen_names.txt\", names=column_names)\n",
    "tabular_data = tabular_data.groupby('patient_id').count().reset_index()\n",
    "tabular_data.sort_values('phenotype', ascending=False)\n",
    "print(tabular_data['phenotype'].mean())\n",
    "print(tabular_data['phenotype'].median())\n",
    "tabular_data.boxplot(column='phenotype')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
